{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54949857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Not with my name! Inferring artists' names of input strings employed by Diffusion Models\n",
      "\n",
      "Roberto Leotta 1[0000 -0003 -4441 -8313] , Oliver Giudice 2[0000 -0002 -8343 -2049] , Luca Guarnera 3[0000 -0001 -8315 -351 X ] , and Sebastiano Battiato 1 3[0000 , -0001 -6127 -2470]\n",
      "\n",
      "- 1 iCTLab Spinoff of University of Catania, Italy\n",
      "- 3 Department of Mathematics and Computer Science, University of Catania, Italy roberto.leotta@ictlab.srl , oliver.giudice@bancaditalia.it , {luca.guarnera, sebastiano.battiato}@unict.it\n",
      "- 2 Applied Research Team, IT dept., Banca d'Italia, Italy\n",
      "\n",
      "Abstract. Diffusion Models (DM) are highly effective at generating realistic, high-quality images. However, these models lack creativity and merely compose outputs based on their training data, guided by a textual input provided at creation time. Is it acceptable to generate images reminiscent of an artist, employing his name as input? This imply that if the DM is able to replicate an artist's work then it was trained on some or all of his artworks thus violating copyright. In this paper, a preliminary study to infer the probability of use of an artist's name in the input string of a generated image is presented. To this aim we focused only on images generated by the famous DALL-E 2 and collected images (both original and generated) of five renowned artists. Finally, a dedicated Siamese Neural Network was employed to have a first kind of probability. Experimental results demonstrate that our approach is an optimal starting point and can be employed as a prior for predicting a complete input string of an investigated image. Dataset and code are available at: https://github.com/ictlab-unict/not-with-my-name.\n",
      "\n",
      "Keywords: Diffusion Models · Artist Recognition · Multimedia Forensics.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "The rapid advancement of generative models, particularly Diffusion Models [7], has led to a surge in high-quality, realistic image generation. These models have demonstrated immense potential for creative applications across various domains, including art, design, and advertising. However, their ability to replicate the styles of specific artists raises concerns about Intellectual Property (IP) rights and potential copyright infringements 4 [20,25,19].\n",
      "\n",
      "4 https://www.theverge.com/2023/1/16/23557098/generative-ai-art-copyright-legallawsuit-stable-diffusion-midjourney-deviantart\n",
      "\n",
      "As the boundary between human creativity and machine-generated content becomes increasingly blurred, it is crucial to address the legal and ethical implications of using generative models to produce art, as well as to develop methods that evaluate the extent to which generated images are influenced by the works of real artists and ensure the protection of their intellectual property.\n",
      "\n",
      "Several studies and articles have explored the legal ramifications of generative models and their potential to infringe on copyrights 5 . In an article dated 2021 the challenges in determining copyright ownership for AI-generated works were discussed and authors emphasized the need for legal frameworks that could adequately address the unique nature of generative models 6 . MLQ.AI also reported on a copyright infringement case involving generative AI 7 , which sparked debates on the responsibilities of AI developers and users in protecting original creators' rights. In response to these concerns, legal scholars have delved into the complexities of copyright law as it pertains to AI-generated artworks. Gillotte [8] examined the challenges in assigning liability and protecting IP rights, arguing that existing copyright laws may not be sufficient to address the unique characteristics of AI-generated contents. Indeed, it is became a copyright dilemma and the need for a balance between innovation and IP protection is arising to ensure that creative works are safeguarded without stifling technological advancements [16].\n",
      "\n",
      "It is not easy to determine how images generated by tools like DALL-E 2 or Midjourney are created by combination of images employed at training time. But, it could useful to develop tools able to deduce the textual prompts that generated an investigated image. Recently, a Kaggle competition was launched on the task 8 but it still lacks of effective methods. However, to make images that appear with an artist's style the prompt of the generating tools should necessary contain the artist's name. Indeed, the generating tool was trained with original artworks belonging to that artist, coupled (as labels) with sentences containing the artist's name. This should demonstrate at a certain level the use of artist's artworks at training time thus violating copyright. Moreover, also the use of the name of a person without his consent should arise other issues.\n",
      "\n",
      "In order make a starting point in the state of the art for techniques able to protect artists, in this study, an introductory empirical analysis method is presented to infer if an artist's name was employed to generate an investigated image. To simplify the problem, an extremely constrained scenario was built by collecting a dataset composed of original artworks from five renowned artists and also images generated employing the OpenAI's DALL-E 2 [17] with artists' names employed as prompted textual strings. Starting from these data a Siamese Neural Network was exploited to learn a dedicated metric for the purpose. Through a series of experiments, the effectiveness of the proposed approach\n",
      "\n",
      "5 https://www.theverge.com/23444685/generative-ai-copyright-infringement-legalfair-use-training-data\n",
      "\n",
      "7 https://www.mlq.ai/copyright-infringement-generative-ai-this-week-in-ai/\n",
      "\n",
      "6 https://www.oreilly.com/radar/what-does-copyright-say-about-generative-models/\n",
      "\n",
      "8 https://www.kaggle.com/competitions/stable-diffusion-image-to-prompts/data\n",
      "\n",
      "was demonstrated in identifying a sort of probability of the usage of an artist's name in the generation process of an image. This study can be extremely important in forensics in order to reconstruct and analyze the history of multimedia content [3].\n",
      "\n",
      "The remainder of this paper is organized as follows: Section 2 lists research papers on the topic introducing the reader to the ethical and IP problems; Section 3 presents the collected dataset with details on composition and how it was built; Section 4 describes the proposed approach starting from a discriminative solution (Section 4.1) to the final metric objective of the study (Section 4.2). Experimental results are presented and discussed in Section 5 and Section 6 concludes the paper with some hints for future development.\n",
      "\n",
      "## 2 Related Works\n",
      "\n",
      "Generative models are a type of machine learning model that use data generation techniques to create new data instances from an existing dataset. These models can be used to generate images, text, sound and other types of data, and have multiple applications in fields such as content creation, simulation and new product creation. Mainly there are two large families of modern generative techniques: the Generative Adversarial Networks (GAN) [11] and the Diffusion Models (DM) [7]. The latter are ultimately being employed by successful applications given their simplicity with which they allow the user to control how the multimedia content has to be generated.\n",
      "\n",
      "Regarding the intellectual property of the data on which generative models have been trained, it can be an important issue as these models can be trained on large amounts of data that belong to other owners, such as images or texts on the internet. If so, the use of this data by generative models could be considered an infringement of copyright or other forms of intellectual property [1].\n",
      "\n",
      "Furthermore, since generative models use training data to learn and create new data, they can also create potential cybersecurity and privacy issues. For example, if a generative model was trained on individuals' personal data, it could generate new personal information that could be used for improper purposes.\n",
      "\n",
      "Given the potential risks of generated multimedia contents, several state of the art works already addressed the issue by trying to detect if a query sample is real or fake [24,22]) or even recognize the specific GAN architecture that generated it [9,23,13,14]. Contents generated by DMs are already being investigated and there are some works that try to expose them [18,6].\n",
      "\n",
      "However, detecting a synthetic content is just the first step. When it comes to evaluate if a certain image infringes IP property different approaches have to be developed. Yet while the problem is only discussed in newspapers first research papers are being published.\n",
      "\n",
      "The most interesting one is the work of Carlini et al [5] which analyses different DMs in order to infer if a certain image was employed in the training set given a set of constraining hypothesis. This is a great starting point giving hints on possible discriminating features, however it only addresses privacy issues.\n",
      "\n",
      "Table 1. Number of images employed. From the second to the last column the artists' names are given. The last two rows describe the total number of synthetic and real original images collected for each artist.\n",
      "\n",
      "| Alfred Sisley Claude Monet Pablo Picasso Paul Cezanne Pierre-Auguste Renoir   | Alfred Sisley Claude Monet Pablo Picasso Paul Cezanne Pierre-Auguste Renoir   | Alfred Sisley Claude Monet Pablo Picasso Paul Cezanne Pierre-Auguste Renoir   | Alfred Sisley Claude Monet Pablo Picasso Paul Cezanne Pierre-Auguste Renoir   | Alfred Sisley Claude Monet Pablo Picasso Paul Cezanne Pierre-Auguste Renoir   | Alfred Sisley Claude Monet Pablo Picasso Paul Cezanne Pierre-Auguste Renoir   |\n",
      "|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|\n",
      "| #Synthetic                                                                    | 1,722                                                                         | 1,702                                                                         | 1,515                                                                         | 1,734                                                                         | 1,846                                                                         |\n",
      "| #Original                                                                     | 470                                                                           | 1,044                                                                         | 1,019                                                                         | 588                                                                           | 1,008                                                                         |\n",
      "\n",
      "A method to effectively protect the IP is yet to be proposed. This work is a first attempt at predicting if a specific text (i.e. artists's names) was employed as input prompt at generation time.\n",
      "\n",
      "## 3 A dataset of original and synthetic artworks\n",
      "\n",
      "Three different dedicated datasets were collected to evaluate the effectiveness of modern Diffusion Models in generating images of specific artists and to have a basis to train the solutions proposed in this paper.\n",
      "\n",
      "The first two datasets, containing synthetic and original data respectively, were created as described below.\n",
      "\n",
      "Five prominent artists were selected to generate the synthetic images: Alfred Sisley, Claude Monet, Pablo Picasso, Paul Cezanne and Pierre-Auguste Renoir. For each artist, 2 000 , synthetic images with a size of 512 × 512 pixels, were generated using DALL-E 2 and employing 1 000 , different descriptive text lines similar to the following: A field of sunflowers with a blue sky background ; A group of children playing at a playground in a city park ; A group of elephants drinking at a watering hole in the savannah .\n",
      "\n",
      "To ensure that the generated images had the same style as the selected artist, the text \" by Artist's name \" was added to each line of input prompting text, such as follows: A field of sunflowers with a blue sky background, by Pablo Picasso ; A group of children playing at a playground in a city park, by Alfred Sisley ; A group of elephants drinking at a watering hole in the savannah, by Claude Monet .\n",
      "\n",
      "In this way, two images per artist were generated for each text line. In addition, different context sentences were used for each artist, thus ensuring a diverse representation of the artist's style across subjects and maximizing variability.\n",
      "\n",
      "In order to work with synthetic data representing the style of the involved artists, validation and cleaning operations of the generated dataset were necessary because the DALL-E 2 algorithm also produces images that did not match the intended artistic style or content (e.g., photo-realistic images rather than paintings).\n",
      "\n",
      "In addition, original paintings of the same five artists were also downloaded from WikiArt 9 . Table 1 shows the total number of original and synthetic images used in the experimental phase. The original images have different resolutions and, as shown in the Table 1, some artists have a limited number of works.\n",
      "\n",
      "9 www.wikiart.org\n",
      "\n",
      "Fig. 1. Proposed approach. The dataset of artists (real and synthetic) was used in the Baseline approach (1) to discriminate against artist. To achieve more generalization, explainability and best define the similarity between different artist styles (2), a Siamese engine was trained from the pretrained model of (1). The ResNET-18 architecture was used in all experiments.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Finally, a third mixed dataset was created, consisting of 50% original images and 50% synthetic images. To obtain a balanced dataset, 470 images from the synthetic image dataset and 470 images from the real image dataset for each artist were randomly selected. This choice was made as Alfred Sisley has 470 images in the real image dataset.\n",
      "\n",
      "The three datasets were split into two parts: 80% as training set and the remaining 20% of samples as validation set.\n",
      "\n",
      "## 4 The proposed approach\n",
      "\n",
      "The ultimate goal of this study was to develop a metric, a sort of probability that a content generated by OpenAI's DALL-E 2 was obtained prompting the tool with a sentence containing the name of one of the artists to be protected.\n",
      "\n",
      "As a starting point, let D be the reference dataset to be protected containing a finite number of authors and their artworks. The presented metric will evaluate the query image by means of comparison with all the elements in D . Specifically, the objective is to estimate a set of distances L = ̂ d I, s ( ) where I is the image to be investigated and s ∈ D is an original image of the artists taken into account. The ̂ d function is not known and has to be modelled in such a way that L has to be close to 0 if the two considered samples belong to the same author (both being original or synthetic and generated with his name). On the contrary, values of L should be 1 or bigger if the two considered samples are unluckily related.\n",
      "\n",
      "At first, an evaluation study of a method to classify images between authors with the aim to learn the discriminative features was carried out. Further analysis was then carried out in order to model the final metric by means of a Siamese Neural Network (taking inspiration from [10,12,2] with the objective of learning a distance ̂ d . The overall study could be schematized as described in Figure 1.\n",
      "\n",
      "## 4.1 Learning to discriminate\n",
      "\n",
      "In order to demonstrate the possibility of discriminating the authors considered in our dataset D . Taking inspiration from [18,6], a Resnet-18 [15] was employed.\n",
      "\n",
      "The first training experiment was to assess the Resnet-18 ability to discriminate between the different artists employing the original artworks only as training data. This set of images was split into 80% for training and 20% for testing purposes. After training the Resnet-18 model, excellent results in discriminating the artists were obtained on the original images meaning that the model successfully captured the distinctive characteristics of each artist's style, which was reflected in the high accuracy achieved during testing.However, the trained Resnet-18 model applied on the synthetic images, was not able to obtain similar good results as shown in Figure 2(a). The model performances significantly deteriorated, with only a few artists being discriminated with reasonable accuracy. This indicated that the model struggled to generalize its understanding of the artists' styles for the synthetic images. It is clear that this transfer learning approach was not able to take into account features extractable from synthetic images, probably because of the generative process itself [13].\n",
      "\n",
      "Given that the before-mentioned transfer learning attempt was not able to correctly discriminate synthetic images, a new Resnet-18 model was trained with the mixed dataset described in Section 3 (with the common train-test split 80%20% as well). This time, the results obtained were way better then previous one so it can be said that this last Resnet-18 model learned not only features about authors' styles but also how DALL-E 2 alters images invisibly (i.e. by producing different distributions of frequencies of the generated images with respect to original ones). Upon analyzing the confusion matrix (Figure 2(b)), it was possible to find an higher entropy for those artists with fewer publicly available works in the dataset. This could give an hint about DALLE-E training phase: it might have been trained on a dataset with a similar distribution and cardinality as the employed dataset D , which led to the generative model's inability to specialize in the artistic styles of specific artists with limited available samples. Obviously this is a strong assumption given that it is a just an introductory, trivial and preliminary insight, which could lead to further investigation on the composition of the dataset employed by the DM.\n",
      "\n",
      "Given the obtained results, for the final objective of this paper this last model will be employed for further investigations described in the next Section.\n",
      "\n",
      "## 4.2 Learning a metric for mixed artworks\n",
      "\n",
      "While the goal of a single Resnet-18 was to learn a hierarchy of feature representations to solve the discriminative tasks between 5 classes (the five considered artists), a Siamese Neural Network can be exploited for weakly supervised metric learning tasks. Instead of taking single sample as input, the network takes a pair of samples, and the loss functions are usually defined over pairs. In this case the loss function of a pair has the following form:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "(b)\n",
      "\n",
      "Fig. 2. (a) Transfer learning attempt. Results are shown as a confusion matrix computed on synthetic images with the Resnet-18 model trained only on original artworks. (b) Confusion matrix obtained on test set with the Resnet-18 model trained only on mixed dataset containing both original and synthetic images.\n",
      "\n",
      "Fig. 3. Training losses of the siamese architecture. Orange shows the trend on the training set while blue is the validation one.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "where s 1 and s 2 are two pair of samples, y ∈ 0 1 , is the similarity label, and D w is a distance function defined as in [21]. Parameters were set as follows α = 1 /C p , β = C n and γ = -2 77 . /C n where C p = 0 2 . and C n = 10 .\n",
      "\n",
      "Unlike methods that assign binary similarity labels to pairs, the network aims at bringing the output feature vectors closer for input pairs of the training labeled as similar, or push the feature vectors away if the input pairs are labeled as dissimilar.\n",
      "\n",
      "Figure 1 shows the overall architecture showing as baselines the Resnet18 models as trained and described in previous Section. The siamese network was trained employing the same training set used to previously train the single Resnet-18 models from the mixed dataset. Training and validation loss are shown in Figure 3. Best model was considered for experiments with respect to the elbow of validation loss as shown in Figure 3.\n",
      "\n",
      "Given the properties of the loss L learned by the siamese network in a context of balances classes, the trained distance ̂ d is a likelihood function describing the possibility to have image I being generated by one of the five authors' names took into account[4]. Thus, it is possible to define the probability that image I was generated by the tool DALL-E 2, by prompting a string containing an artist's name a between the five considered:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where ̂ d is the distance obtained by the trained siamese network, s is the set of images generated by DALL-E 2 with the corresponding author name a . In order to have P ∈ [0 , 1] , eq. 2 is evaluated only when min d I, s ( ̂ ( a )) ≤ 1 . Figure 4 shows graphically the proposed final approach\n",
      "\n",
      "Fig. 4. Final approach. Given an input image to ResNET-18, the similarity score (defined by the Siamese approach) with respect to each real image of the involved artists is calculated. A voting process will define the number of images for each artist whose similarity score S exceeds a set threshold value T. Then, the test image will be assigned to a specific artist with respect to the highest score obtained in the previous step.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## 5 Experiments and discussion\n",
      "\n",
      "The siamese neural network with a baseline Resnet-18 model specialized in discriminating authors was developed for the purposes of this study as described in previous Sections. The Resnet-18 discriminative model was trained using the following parameters: ADAM optimization, 50 epochs, batch size of 32, weight decay and LR of 0.0001. The siamese configuration was trained with the loss as defined in Equation 1, ADAM optimization, 250 epochs, batch size of 64, weight decay and lr of 0.0001 and contrastive loss margin of 2.\n",
      "\n",
      "In order to evaluate the effectiveness of the method a retrieval test was carried out. The retrieval test is able to show how the metric is working in correcting detecting the author by means of comparison between query images and all original artworks in the test set of the mixed dataset D .\n",
      "\n",
      "For each author a single generated image was selected as query and all the original artworks samples were employed to perform the retrieval test. The retrieval performance has been evaluated with the probability of the successful retrieval P n ( ) in a number of test queries P n ( ) = Q /Q n , where Q n is the number of successful queries according to topn criterion, i.e., the correct classification is among the first n retrieved images, and Q is the total number of queries. The average of P n ( ) values with respect to all queries by considering only images achieving a distance ̂ d ≤ T where T ∈ { 0 1 . , 0 2 . , 0 3 . , 0 4 . , 0 5 . } is reported in Figure 5 at varying of n .\n",
      "\n",
      "It has to be noted that images that are very close in terms of ̂ d are not similar in terms of aspect, style, contents and semantics. This empirically implies that ̂ d is not working on those features but learned a sort of probability of having the author name as input prompt at generation time: the only thing that unites all the artworks of an artist.\n",
      "\n",
      "The results obtained demonstrate that the proposed siamese approach could be employed to protect any kind of artists database given that a re-training\n",
      "\n",
      "Fig. 5. Retrieval test results with different threshold values for the distance ̂ d at varying of the top-n retrieved samples.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Fig. 6. An example with a query image I (the generated one) and the real closest image found by means of the learned metric. As an overall result, a probability is obtained that the name Picasso was used for the generation of image I .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "phase is needed to a certain amount of images generated by prompting their name. Figure 6 shows a real case scenario. Generalization could be achieved only by means of integrating more samples in the dataset with both original images\n",
      "\n",
      "and generated ones. Finally, it has to be noted that this work was carried out only on images generated with DALL-E limiting the generalization of the results for other DMs or tools available. While probably being robust with other images generated with DM solutions, it surely would not generalize on GANs being the already detected invisible differences extractable from images [6].\n",
      "\n",
      "## 6 Conclusion and future works\n",
      "\n",
      "In this paper, a novel approach to estimate the probability of an artist's name being used in the input prompt of an image generated by a diffusion model, specifically DALL-E 2, was presented. Our approach aimed to address the concern of potential intellectual property infringement in the context of image generation, as the usage of an artist's name in the input string might imply that the diffusion model has learned from some or all of the artist's work, potentially violating their copyright.\n",
      "\n",
      "This work employed metric learning for classification of an extremely limited number of authors but in future more sophisticated similarity measures, larger and more diverse datasets, and additional techniques to refine the estimation of the input string's content could be explored. Additionally, investigating the ethical implications and possible solutions to the challenges posed by AI-generated content in relation to copyright and intellectual property protection should be a priority for the research community.\n",
      "\n",
      "## References\n",
      "\n",
      "- 1. Abbott, R.: Intellectual property and artificial intelligence: an introduction. In: Research Handbook on Intellectual Property and Artificial Intelligence, pp. 2-21. Edward Elgar Publishing (2022)\n",
      "- 3. Battiato, S., Giudice, O., Paratore, A.: Multimedia forensics: discovering the history of multimedia contents. In: Proceedings of the 17th International Conference on Computer Systems and Technologies 2016. pp. 5-16 (2016)\n",
      "- 2. Battiato, S., Giudice, O., Guarnera, F., Puglisi, G.: Cnn-based first quantization estimation of double compressed jpeg images. Journal of Visual Communication and Image Representation 89 , 103635 (2022)\n",
      "- 4. Berlemont, S., Lefebvre, G., Duffner, S., Garcia, C.: Class-balanced siamese neural networks. Neurocomputing 273 , 47-56 (2018)\n",
      "- 6. Corvi, R., Cozzolino, D., Zingarini, G., Poggi, G., Nagano, K., Verdoliva, L.: On the Detection of Synthetic Images Generated by Diffusion Models. arXiv preprint arXiv:2211.00680 (2022)\n",
      "- 5. Carlini, N., Hayes, J., Nasr, M., Jagielski, M., Sehwag, V., Tramèr, F., Balle, B., Ippolito, D., Wallace, E.: Extracting training data from diffusion models. arXiv preprint arXiv:2301.13188 (2023)\n",
      "- 7. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems 34 , 8780-8794 (2021)\n",
      "- 8. Gillotte, J.L.: Copyright infringement in ai-generated artworks. UC Davis L. Rev. 53 , 2655 (2019)\n",
      "- 12 Roberto Leotta, Oliver Giudice, Luca Guarnera, and Sebastiano Battiato\n",
      "- 9. Giudice, O., Guarnera, L., Battiato, S.: Fighting Deepfakes by Detecting GAN DCTAnomalies. Journal of Imaging 7 (8), 128 (2021). https://doi.org/10.3390/ jimaging7080128 , https://www.mdpi.com/2313-433X/7/8/128\n",
      "- 11. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative Adversarial Nets. In: Advances in Neural Information Processing Systems. pp. 2672-2680 (2014)\n",
      "- 10. Giudice, O., Guarnera, L., Paratore, A.B., Farinella, G.M., Battiato, S.: Siamese ballistics neural network. In: 2019 IEEE International Conference on Image Processing (ICIP). pp. 4045-4049. IEEE (2019)\n",
      "- 12. Guarnera, F., Allegra, D., Giudice, O., Stanco, F., Battiato, S.: A new study on wood fibers textures: documents authentication through lbp fingerprint. In: 2019 IEEE International Conference on Image Processing (ICIP). pp. 4594-4598. IEEE (2019)\n",
      "- 13. Guarnera, L., Giudice, O., Battiato, S.: Fighting Deepfake by Exposing the Convolutional Traces on Images. IEEE Access 8 , 165085-165098 (2020). https: //doi.org/10.1109/ACCESS.2020.3023037\n",
      "- 14. Guarnera, L., Giudice, O., Guarnera, F., Ortis, A., Puglisi, G., Paratore, A., Bui, L.M., Fontani, M., Coccomini, D.A., Caldelli, R., et al.: The face deepfake detection challenge. Journal of Imaging 8 (10), 263 (2022)\n",
      "- 16. Hristov, K.: Artificial intelligence and the copyright dilemma. Idea 57 , 431 (2016)\n",
      "- 15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 770-778 (2016)\n",
      "- 17. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical TextConditional Image Generation with Clip Latents. arXiv preprint arXiv:2204.06125 (2022)\n",
      "- 19. Shan, S., Cryan, J., Wenger, E., Zheng, H., Hanocka, R., Zhao, B.Y.: Glaze: Protecting artists from style mimicry by text-to-image models. arXiv preprint arXiv:2302.04222 (2023)\n",
      "- 18. Sha, Z., Li, Z., Yu, N., Zhang, Y.: DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Diffusion Models. arXiv preprint arXiv:2210.06998 (2022)\n",
      "- 20. Vyas, N., Kakade, S., Barak, B.: Provable copyright protection for generative models. arXiv preprint arXiv:2302.10870 (2023)\n",
      "- 22. Wang, R., Juefei-Xu, F., Ma, L., Xie, X., Huang, Y., Wang, J., Liu, Y.: Fakespotter: A Simple Yet Robust Baseline for Spotting AI-Synthesized Fake Faces. arXiv preprint arXiv:1909.06122 (2019)\n",
      "- 21. Wang, F., Kang, L., Li, Y.: Sketch-based 3d shape retrieval using convolutional neural networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1875-1883 (2015)\n",
      "- 23. Wang, S.Y., Wang, O., Zhang, R., Owens, A., Efros, A.A.: CNN-Generated Images are Surprisingly Easy to Spot... for Now. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8695-8704 (2020)\n",
      "- 25. Zirpoli, C.T.: Generative artificial intelligence and copyright law (2023)\n",
      "- 24. Zhang, X., Karaman, S., Chang, S.F.: Detecting and Simulating Artifacts in GAN Fake Images. In: 2019 IEEE International Workshop on Information Forensics and Security (WIFS). pp. 1-6. IEEE (2019)\n"
     ]
    }
   ],
   "source": [
    "# Apri il file in modalità lettura\n",
    "with open('result_document.md', 'r') as file:\n",
    "    # Leggi il contenuto del file\n",
    "    contenuto = file.read()\n",
    "\n",
    "# Stampa il contenuto del file\n",
    "print(contenuto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f007a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# 1. Pulizia del testo\n",
    "import re\n",
    "def clean_text(raw_text):\n",
    "    text = raw_text\n",
    "    text = re.sub(r'\\s+', ' ', text)  # rimuove spazi multipli\n",
    "    text = re.sub(r'[^\\w\\s.,;!?]', '', text)  # rimuove simboli speciali\n",
    "    return text.strip()\n",
    "\n",
    "clean_text(contenuto)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f3b07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /storage/data_4T_b/andreacutu\n",
      "[nltk_data]     li/PROVA/nome_ambiente/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /storage/data_4T_b/andrea\n",
      "[nltk_data]     cutuli/PROVA/nome_ambiente/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Tokenizzazione\n",
    "import nltk\n",
    "#conda install -c conda-forge nltk\n",
    "\n",
    "# Definisci un percorso specifico per i dati\n",
    "nltk.data.path.append(\"/storage/data_4T_b/andreacutuli/PROVA/nome_ambiente/nltk_data\")\n",
    "\n",
    "# Scarica il corpus 'punkt'\n",
    "nltk.download('punkt', download_dir=\"/storage/data_4T_b/andreacutuli/PROVA/nome_ambiente/nltk_data\")\n",
    "nltk.download('punkt_tab', download_dir=\"/storage/data_4T_b/andreacutuli/PROVA/nome_ambiente/nltk_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fec7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(clean_text(contenuto))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60179844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/data_4T_b/andreacutuli/miniconda3/envs/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#3. Raggruppamento per similarità semantica\n",
    "#!conda install -c conda-forge sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# clustering con soglia o algoritmo\n",
    "clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0.5)\n",
    "labels = clustering.fit_predict(embeddings)\n",
    "\n",
    "# creare i chunk\n",
    "from collections import defaultdict\n",
    "chunks = defaultdict(list)\n",
    "for sentence, label in zip(sentences, labels):\n",
    "    chunks[label].append(sentence)\n",
    "\n",
    "chunks = [\" \".join(chunk) for chunk in chunks.values()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ae76c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers è stato installato correttamente!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"sentence-transformers è stato installato correttamente!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf666472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Several studies and articles have explored the legal ramifications of generative models and their potential to infringe on copyrights 5 .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Seleziona un chunk a caso\n",
    "random_chunk = random.choice(chunks)\n",
    "\n",
    "# Stampa il chunk selezionato\n",
    "print(random_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89ed7a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3432477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: Not with my name!\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Inferring artists names of input strings employed by Diffusion Models Roberto Leotta 10000 0003 4441 8313 , Oliver Giudice 20000 0002 8343 2049 , Luca Guarnera 30000 0001 8315 351 X  , and Sebastiano Battiato 1 30000 , 0001 6127 2470  1 iCTLab Spinoff of University of Catania, Italy  3 Department of Mathematics and Computer Science, University of Catania, Italy roberto.leottaictlab.srl , oliver.giudicebancaditalia.it , luca.guarnera, sebastiano.battiatounict.it  2 Applied Research Team, IT dept., Banca dItalia, Italy Abstract.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Diffusion Models DM are highly effective at generating realistic, highquality images.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: However, these models lack creativity and merely compose outputs based on their training data, guided by a textual input provided at creation time.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Is it acceptable to generate images reminiscent of an artist, employing his name as input?\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: This imply that if the DM is able to replicate an artists work then it was trained on some or all of his artworks thus violating copyright.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: In this paper, a preliminary study to infer the probability of use of an artists name in the input string of a generated image is presented.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: To this aim we focused only on images generated by the famous DALLE 2 and collected images both original and generated of five renowned artists.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Finally, a dedicated Siamese Neural Network was employed to have a first kind of probability.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Experimental results demonstrate that our approach is an optimal starting point and can be employed as a prior for predicting a complete input string of an investigated image.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Dataset and code are available at httpsgithub.comictlabunictnotwithmyname.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Keywords Diffusion Models  Artist Recognition  Multimedia Forensics.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: 1 Introduction The rapid advancement of generative models, particularly Diffusion Models 7, has led to a surge in highquality, realistic image generation.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: These models have demonstrated immense potential for creative applications across various domains, including art, design, and advertising.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: However, their ability to replicate the styles of specific artists raises concerns about Intellectual Property IP rights and potential copyright infringements 4 20,25,19.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: 4 httpswww.theverge.com202311623557098generativeaiartcopyrightlegallawsuitstablediffusionmidjourneydeviantart As the boundary between human creativity and machinegenerated content becomes increasingly blurred, it is crucial to address the legal and ethical implications of using generative models to produce art, as well as to develop methods that evaluate the extent to which generated images are influenced by the works of real artists and ensure the protection of their intellectual property.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Several studies and articles have explored the legal ramifications of generative models and their potential to infringe on copyrights 5 .\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: In an article dated 2021 the challenges in determining copyright ownership for AIgenerated works were discussed and authors emphasized the need for legal frameworks that could adequately address the unique nature of generative models 6 .\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: MLQ.AI also reported on a copyright infringement case involving generative AI 7 , which sparked debates on the responsibilities of AI developers and users in protecting original creators rights.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: In response to these concerns, legal scholars have delved into the complexities of copyright law as it pertains to AIgenerated artworks.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Gillotte 8 examined the challenges in assigning liability and protecting IP rights, arguing that existing copyright laws may not be sufficient to address the unique characteristics of AIgenerated contents.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Indeed, it is became a copyright dilemma and the need for a balance between innovation and IP protection is arising to ensure that creative works are safeguarded without stifling technological advancements 16.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: It is not easy to determine how images generated by tools like DALLE 2 or Midjourney are created by combination of images employed at training time.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: But, it could useful to develop tools able to deduce the textual prompts that generated an investigated image.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Recently, a Kaggle competition was launched on the task 8 but it still lacks of effective methods.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: However, to make images that appear with an artists style the prompt of the generating tools should necessary contain the artists name.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Indeed, the generating tool was trained with original artworks belonging to that artist, coupled as labels with sentences containing the artists name.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: This should demonstrate at a certain level the use of artists artworks at training time thus violating copyright.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Moreover, also the use of the name of a person without his consent should arise other issues.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: In order make a starting point in the state of the art for techniques able to protect artists, in this study, an introductory empirical analysis method is presented to infer if an artists name was employed to generate an investigated image.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: To simplify the problem, an extremely constrained scenario was built by collecting a dataset composed of original artworks from five renowned artists and also images generated employing the OpenAIs DALLE 2 17 with artists names employed as prompted textual strings.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Starting from these data a Siamese Neural Network was exploited to learn a dedicated metric for the purpose.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Through a series of experiments, the effectiveness of the proposed approach 5 httpswww.theverge.com23444685generativeaicopyrightinfringementlegalfairusetrainingdata 7 httpswww.mlq.aicopyrightinfringementgenerativeaithisweekinai 6 httpswww.oreilly.comradarwhatdoescopyrightsayaboutgenerativemodels 8 httpswww.kaggle.comcompetitionsstablediffusionimagetopromptsdata was demonstrated in identifying a sort of probability of the usage of an artists name in the generation process of an image.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: This study can be extremely important in forensics in order to reconstruct and analyze the history of multimedia content 3.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: The remainder of this paper is organized as follows Section 2 lists research papers on the topic introducing the reader to the ethical and IP problems; Section 3 presents the collected dataset with details on composition and how it was built; Section 4 describes the proposed approach starting from a discriminative solution Section 4.1 to the final metric objective of the study Section 4.2.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Experimental results are presented and discussed in Section 5 and Section 6 concludes the paper with some hints for future development.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: 2 Related Works Generative models are a type of machine learning model that use data generation techniques to create new data instances from an existing dataset.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: These models can be used to generate images, text, sound and other types of data, and have multiple applications in fields such as content creation, simulation and new product creation.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Mainly there are two large families of modern generative techniques the Generative Adversarial Networks GAN 11 and the Diffusion Models DM 7.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: The latter are ultimately being employed by successful applications given their simplicity with which they allow the user to control how the multimedia content has to be generated.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Regarding the intellectual property of the data on which generative models have been trained, it can be an important issue as these models can be trained on large amounts of data that belong to other owners, such as images or texts on the internet.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: If so, the use of this data by generative models could be considered an infringement of copyright or other forms of intellectual property 1.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Furthermore, since generative models use training data to learn and create new data, they can also create potential cybersecurity and privacy issues.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: For example, if a generative model was trained on individuals personal data, it could generate new personal information that could be used for improper purposes.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Given the potential risks of generated multimedia contents, several state of the art works already addressed the issue by trying to detect if a query sample is real or fake 24,22 or even recognize the specific GAN architecture that generated it 9,23,13,14.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Contents generated by DMs are already being investigated and there are some works that try to expose them 18,6.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: However, detecting a synthetic content is just the first step.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: When it comes to evaluate if a certain image infringes IP property different approaches have to be developed.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Yet while the problem is only discussed in newspapers first research papers are being published.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: The most interesting one is the work of Carlini et al 5 which analyses different DMs in order to infer if a certain image was employed in the training set given a set of constraining hypothesis.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: This is a great starting point giving hints on possible discriminating features, however it only addresses privacy issues.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Table 1.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Number of images employed.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: From the second to the last column the artists names are given.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: The last two rows describe the total number of synthetic and real original images collected for each artist.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Alfred Sisley Claude Monet Pablo Picasso Paul Cezanne PierreAuguste Renoir  Alfred Sisley Claude Monet Pablo Picasso Paul Cezanne PierreAuguste Renoir  Alfred Sisley Claude Monet Pablo Picasso Paul Cezanne PierreAuguste Renoir  Alfred Sisley Claude Monet Pablo Picasso Paul Cezanne PierreAuguste Renoir  Alfred Sisley Claude Monet Pablo Picasso Paul Cezanne PierreAuguste Renoir  Alfred Sisley Claude Monet Pablo Picasso Paul Cezanne PierreAuguste Renoir    Synthetic  1,722  1,702  1,515  1,734  1,846   Original  470  1,044  1,019  588  1,008  A method to effectively protect the IP is yet to be proposed.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: This work is a first attempt at predicting if a specific text i.e.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: artistss names was employed as input prompt at generation time.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: 3 A dataset of original and synthetic artworks Three different dedicated datasets were collected to evaluate the effectiveness of modern Diffusion Models in generating images of specific artists and to have a basis to train the solutions proposed in this paper.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: The first two datasets, containing synthetic and original data respectively, were created as described below.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Five prominent artists were selected to generate the synthetic images Alfred Sisley, Claude Monet, Pablo Picasso, Paul Cezanne and PierreAuguste Renoir.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: For each artist, 2 000 , synthetic images with a size of 512  512 pixels, were generated using DALLE 2 and employing 1 000 , different descriptive text lines similar to the following A field of sunflowers with a blue sky background ; A group of children playing at a playground in a city park ; A group of elephants drinking at a watering hole in the savannah .\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: To ensure that the generated images had the same style as the selected artist, the text  by Artists name  was added to each line of input prompting text, such as follows A field of sunflowers with a blue sky background, by Pablo Picasso ; A group of children playing at a playground in a city park, by Alfred Sisley ; A group of elephants drinking at a watering hole in the savannah, by Claude Monet .\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: In this way, two images per artist were generated for each text line.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: In addition, different context sentences were used for each artist, thus ensuring a diverse representation of the artists style across subjects and maximizing variability.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: In order to work with synthetic data representing the style of the involved artists, validation and cleaning operations of the generated dataset were necessary because the DALLE 2 algorithm also produces images that did not match the intended artistic style or content e.g., photorealistic images rather than paintings.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: In addition, original paintings of the same five artists were also downloaded from WikiArt 9 .\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Table 1 shows the total number of original and synthetic images used in the experimental phase.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: The original images have different resolutions and, as shown in the Table 1, some artists have a limited number of works.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: 9 www.wikiart.org Fig.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: 1.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Proposed approach.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: The dataset of artists real and synthetic was used in the Baseline approach 1 to discriminate against artist.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: To achieve more generalization, explainability and best define the similarity between different artist styles 2, a Siamese engine was trained from the pretrained model of 1.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: The ResNET18 architecture was used in all experiments.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: ! ! ! ! !\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: image  Finally, a third mixed dataset was created, consisting of 50 original images and 50 synthetic images.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: To obtain a balanced dataset, 470 images from the synthetic image dataset and 470 images from the real image dataset for each artist were randomly selected.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: This choice was made as Alfred Sisley has 470 images in the real image dataset.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: The three datasets were split into two parts 80 as training set and the remaining 20 of samples as validation set.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: 4 The proposed approach The ultimate goal of this study was to develop a metric, a sort of probability that a content generated by OpenAIs DALLE 2 was obtained prompting the tool with a sentence containing the name of one of the artists to be protected.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: As a starting point, let D be the reference dataset to be protected containing a finite number of authors and their artworks.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: The presented metric will evaluate the query image by means of comparison with all the elements in D .\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Specifically, the objective is to estimate a set of distances L   d I, s   where I is the image to be investigated and s  D is an original image of the artists taken into account.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: The  d function is not known and has to be modelled in such a way that L has to be close to 0 if the two considered samples belong to the same author both being original or synthetic and generated with his name.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: On the contrary, values of L should be 1 or bigger if the two considered samples are unluckily related.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: At first, an evaluation study of a method to classify images between authors with the aim to learn the discriminative features was carried out.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Further analysis was then carried out in order to model the final metric by means of a Siamese Neural Network taking inspiration from 10,12,2 with the objective of learning a distance  d .\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: The overall study could be schematized as described in Figure 1.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: 4.1 Learning to discriminate In order to demonstrate the possibility of discriminating the authors considered in our dataset D .\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Taking inspiration from 18,6, a Resnet18 15 was employed.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: The first training experiment was to assess the Resnet18 ability to discriminate between the different artists employing the original artworks only as training data.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: This set of images was split into 80 for training and 20 for testing purposes.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: After training the Resnet18 model, excellent results in discriminating the artists were obtained on the original images meaning that the model successfully captured the distinctive characteristics of each artists style, which was reflected in the high accuracy achieved during testing.However, the trained Resnet18 model applied on the synthetic images, was not able to obtain similar good results as shown in Figure 2a.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: The model performances significantly deteriorated, with only a few artists being discriminated with reasonable accuracy.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: This indicated that the model struggled to generalize its understanding of the artists styles for the synthetic images.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: It is clear that this transfer learning approach was not able to take into account features extractable from synthetic images, probably because of the generative process itself 13.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Given that the beforementioned transfer learning attempt was not able to correctly discriminate synthetic images, a new Resnet18 model was trained with the mixed dataset described in Section 3 with the common traintest split 8020 as well.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: This time, the results obtained were way better then previous one so it can be said that this last Resnet18 model learned not only features about authors styles but also how DALLE 2 alters images invisibly i.e.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: by producing different distributions of frequencies of the generated images with respect to original ones.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Upon analyzing the confusion matrix Figure 2b, it was possible to find an higher entropy for those artists with fewer publicly available works in the dataset.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: This could give an hint about DALLEE training phase it might have been trained on a dataset with a similar distribution and cardinality as the employed dataset D , which led to the generative models inability to specialize in the artistic styles of specific artists with limited available samples.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Obviously this is a strong assumption given that it is a just an introductory, trivial and preliminary insight, which could lead to further investigation on the composition of the dataset employed by the DM.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Given the obtained results, for the final objective of this paper this last model will be employed for further investigations described in the next Section.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: 4.2 Learning a metric for mixed artworks While the goal of a single Resnet18 was to learn a hierarchy of feature representations to solve the discriminative tasks between 5 classes the five considered artists, a Siamese Neural Network can be exploited for weakly supervised metric learning tasks.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Instead of taking single sample as input, the network takes a pair of samples, and the loss functions are usually defined over pairs.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: In this case the loss function of a pair has the following form !\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: formulanotdecoded  !\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: image  !\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: image  b Fig. image  Fig.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: 2. a Transfer learning attempt.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Results are shown as a confusion matrix computed on synthetic images with the Resnet18 model trained only on original artworks.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: b Confusion matrix obtained on test set with the Resnet18 model trained only on mixed dataset containing both original and synthetic images.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Fig.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: 3. 4.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Training losses of the siamese architecture.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Orange shows the trend on the training set while blue is the validation one.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: image  where s 1 and s 2 are two pair of samples, y  0 1 , is the similarity label, and D w is a distance function defined as in 21.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Parameters were set as follows α  1 C p , β  C n and γ  2 77 .\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: C n where C p  0 2 .\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: and C n  10 .\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Unlike methods that assign binary similarity labels to pairs, the network aims at bringing the output feature vectors closer for input pairs of the training labeled as similar, or push the feature vectors away if the input pairs are labeled as dissimilar.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Figure 1 shows the overall architecture showing as baselines the Resnet18 models as trained and described in previous Section.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: The siamese network was trained employing the same training set used to previously train the single Resnet18 models from the mixed dataset.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Training and validation loss are shown in Figure 3.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Best model was considered for experiments with respect to the elbow of validation loss as shown in Figure 3.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Given the properties of the loss L learned by the siamese network in a context of balances classes, the trained distance  d is a likelihood function describing the possibility to have image I being generated by one of the five authors names took into account4.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Thus, it is possible to define the probability that image I was generated by the tool DALLE 2, by prompting a string containing an artists name a between the five considered !\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: formulanotdecoded  where  d is the distance obtained by the trained siamese network, s is the set of images generated by DALLE 2 with the corresponding author name a .\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: In order to have P  0 , 1 , eq.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: 2 is evaluated only when min d I, s    a   1 .\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Figure 4 shows graphically the proposed final approach Fig.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Final approach.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Given an input image to ResNET18, the similarity score defined by the Siamese approach with respect to each real image of the involved artists is calculated.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: A voting process will define the number of images for each artist whose similarity score S exceeds a set threshold value T. Then, the test image will be assigned to a specific artist with respect to the highest score obtained in the previous step.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: image   5 Experiments and discussion The siamese neural network with a baseline Resnet18 model specialized in discriminating authors was developed for the purposes of this study as described in previous Sections.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: The Resnet18 discriminative model was trained using the following parameters ADAM optimization, 50 epochs, batch size of 32, weight decay and LR of 0.0001.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: The siamese configuration was trained with the loss as defined in Equation 1, ADAM optimization, 250 epochs, batch size of 64, weight decay and lr of 0.0001 and contrastive loss margin of 2.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: In order to evaluate the effectiveness of the method a retrieval test was carried out.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: The retrieval test is able to show how the metric is working in correcting detecting the author by means of comparison between query images and all original artworks in the test set of the mixed dataset D .\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: For each author a single generated image was selected as query and all the original artworks samples were employed to perform the retrieval test.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: The retrieval performance has been evaluated with the probability of the successful retrieval P n   in a number of test queries P n    Q Q n , where Q n is the number of successful queries according to topn criterion, i.e., the correct classification is among the first n retrieved images, and Q is the total number of queries.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: The average of P n   values with respect to all queries by considering only images achieving a distance  d  T where T   0 1 .\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: , 0 2 .\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: , 0 3 .\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: , 0 4 .\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: , 0 5 .\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: is reported in Figure 5 at varying of n .\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: It has to be noted that images that are very close in terms of  d are not similar in terms of aspect, style, contents and semantics.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: This empirically implies that  d is not working on those features but learned a sort of probability of having the author name as input prompt at generation time the only thing that unites all the artworks of an artist.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: The results obtained demonstrate that the proposed siamese approach could be employed to protect any kind of artists database given that a retraining Fig.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: 5.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Retrieval test results with different threshold values for the distance  d at varying of the topn retrieved samples.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: 6.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: An example with a query image I the generated one and the real closest image found by means of the learned metric.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: As an overall result, a probability is obtained that the name Picasso was used for the generation of image I .\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: image  phase is needed to a certain amount of images generated by prompting their name.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Figure 6 shows a real case scenario.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Generalization could be achieved only by means of integrating more samples in the dataset with both original images and generated ones.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Finally, it has to be noted that this work was carried out only on images generated with DALLE limiting the generalization of the results for other DMs or tools available.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: While probably being robust with other images generated with DM solutions, it surely would not generalize on GANs being the already detected invisible differences extractable from images 6.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: 6 Conclusion and future works In this paper, a novel approach to estimate the probability of an artists name being used in the input prompt of an image generated by a diffusion model, specifically DALLE 2, was presented.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Our approach aimed to address the concern of potential intellectual property infringement in the context of image generation, as the usage of an artists name in the input string might imply that the diffusion model has learned from some or all of the artists work, potentially violating their copyright.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: This work employed metric learning for classification of an extremely limited number of authors but in future more sophisticated similarity measures, larger and more diverse datasets, and additional techniques to refine the estimation of the input strings content could be explored.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Additionally, investigating the ethical implications and possible solutions to the challenges posed by AIgenerated content in relation to copyright and intellectual property protection should be a priority for the research community.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: References  1.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Abbott, R. Intellectual property and artificial intelligence an introduction.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: In Research Handbook on Intellectual Property and Artificial Intelligence, pp.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: 221.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Edward Elgar Publishing 2022  3.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Battiato, S., Giudice, O., Paratore, A. Multimedia forensics discovering the history of multimedia contents.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: In Proceedings of the 17th International Conference on Computer Systems and Technologies 2016. pp.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: 516 2016  2.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Battiato, S., Giudice, O., Guarnera, F., Puglisi, G. Cnnbased first quantization estimation of double compressed jpeg images.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Journal of Visual Communication and Image Representation 89 , 103635 2022  4.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Berlemont, S., Lefebvre, G., Duffner, S., Garcia, C. Classbalanced siamese neural networks.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Neurocomputing 273 , 4756 2018  6.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Corvi, R., Cozzolino, D., Zingarini, G., Poggi, G., Nagano, K., Verdoliva, L. On the Detection of Synthetic Images Generated by Diffusion Models.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: arXiv preprint arXiv2211.00680 2022  5. arXiv preprint arXiv2301.13188 2023  7. arXiv preprint arXiv2204.06125 2022  19. arXiv preprint arXiv2302.04222 2023  18. arXiv preprint arXiv2210.06998 2022  20. arXiv preprint arXiv2302.10870 2023  22.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Carlini, N., Hayes, J., Nasr, M., Jagielski, M., Sehwag, V., Tramèr, F., Balle, B., Ippolito, D., Wallace, E. Extracting training data from diffusion models.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Dhariwal, P., Nichol, A. Diffusion models beat gans on image synthesis.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Advances in Neural Information Processing Systems 34 , 87808794 2021  8.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Gillotte, J.L.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Copyright infringement in aigenerated artworks.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: UC Davis L. Rev.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: 53 , 2655 2019  12 Roberto Leotta, Oliver Giudice, Luca Guarnera, and Sebastiano Battiato  9.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Giudice, O., Guarnera, L., Battiato, S. Fighting Deepfakes by Detecting GAN DCTAnomalies.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Journal of Imaging 7 8, 128 2021. httpsdoi.org10.3390 jimaging7080128 , httpswww.mdpi.com2313433X78128  11.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Goodfellow, I., PougetAbadie, J., Mirza, M., Xu, B., WardeFarley, D., Ozair, S., Courville, A., Bengio, Y. Generative Adversarial Nets.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: In Advances in Neural Information Processing Systems.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: pp. pp. pp. pp. pp. pp. pp.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: 26722680 2014  10.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Giudice, O., Guarnera, L., Paratore, A.B., Farinella, G.M., Battiato, S. Siamese ballistics neural network.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: In 2019 IEEE International Conference on Image Processing ICIP. In 2019 IEEE International Conference on Image Processing ICIP.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: 40454049.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: IEEE 2019  12. IEEE 2019  13. IEEE 2019\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Guarnera, F., Allegra, D., Giudice, O., Stanco, F., Battiato, S. A new study on wood fibers textures documents authentication through lbp fingerprint.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: 45944598.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Guarnera, L., Giudice, O., Battiato, S. Fighting Deepfake by Exposing the Convolutional Traces on Images.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: IEEE Access 8 , 165085165098 2020. https doi.org10.1109ACCESS.2020.3023037  14.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Guarnera, L., Giudice, O., Guarnera, F., Ortis, A., Puglisi, G., Paratore, A., Bui, L.M., Fontani, M., Coccomini, D.A., Caldelli, R., et al.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: The face deepfake detection challenge.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Journal of Imaging 8 10, 263 2022  16.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Hristov, K. Artificial intelligence and the copyright dilemma.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Idea 57 , 431 2016  15.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: He, K., Zhang, X., Ren, S., Sun, J.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Deep residual learning for image recognition.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: 770778 2016  17.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M. Hierarchical TextConditional Image Generation with Clip Latents.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Shan, S., Cryan, J., Wenger, E., Zheng, H., Hanocka, R., Zhao, B.Y.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Glaze Protecting artists from style mimicry by texttoimage models.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Sha, Z., Li, Z., Yu, N., Zhang, Y. DEFAKE Detection and Attribution of Fake Images Generated by TexttoImage Diffusion Models.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Vyas, N., Kakade, S., Barak, B. Provable copyright protection for generative models.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Wang, R., JuefeiXu, F., Ma, L., Xie, X., Huang, Y., Wang, J., Liu, Y. Fakespotter A Simple Yet Robust Baseline for Spotting AISynthesized Fake Faces.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: arXiv preprint arXiv1909.06122 2019  21.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Wang, F., Kang, L., Li, Y. Sketchbased 3d shape retrieval using convolutional neural networks.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: 18751883 2015  23.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Wang, S.Y., Wang, O., Zhang, R., Owens, A., Efros, A.A. CNNGenerated Images are Surprisingly Easy to Spot... for Now.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: In Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: 86958704 2020  25.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Zirpoli, C.T.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Generative artificial intelligence and copyright law 2023  24.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: Zhang, X., Karaman, S., Chang, S.F.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: Detecting and Simulating Artifacts in GAN Fake Images.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n",
      "Chunk: In 2019 IEEE International Workshop on Information Forensics and Security WIFS.\n",
      "Etichetta assegnata: model\n",
      "----------------------------------------\n",
      "Chunk: 16.\n",
      "Etichetta assegnata: artist\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#ASSEGNARE ETICHETTE AI CHUNK DA ETICHETTE PREDIFINITE\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Carica il modello zero-shot classification pre-addestrato\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "\n",
    "# Etichette generiche che il modello potrebbe utilizzare per classificare automaticamente i chunk\n",
    "candidate_labels = [\"paint\", \"model\", \"artist\", \"nature\"]\n",
    "\n",
    "# Assegna l'etichetta per ciascun chunk\n",
    "chunk_labels = []\n",
    "for chunk in chunks:\n",
    "    result = classifier(chunk, candidate_labels)\n",
    "    # Prendi l'etichetta con la probabilità più alta\n",
    "    chunk_labels.append(result['labels'][0])\n",
    "\n",
    "# Visualizza i risultati\n",
    "for chunk, label in zip(chunks, chunk_labels):\n",
    "    print(f\"Chunk: {chunk}\")\n",
    "    print(f\"Etichetta assegnata: {label}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e587dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: Clustering tematico con K-means\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Usa le embedding dei chunk (da Sentence-BERT)\n",
    "# embeddings è già stato calcolato in precedenza\n",
    "embeddings = model.encode(chunks)  # Assicurati che chunks contenga i tuoi chunk\n",
    "\n",
    "# Definisci il numero di cluster\n",
    "num_clusters = 5  # Puoi cambiare questo numero in base al tuo caso\n",
    "\n",
    "# Applica K-means\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Assegna un'etichetta al chunk\n",
    "clustered_chunks = {}\n",
    "for label, chunk in zip(labels, chunks):\n",
    "    if label not in clustered_chunks:\n",
    "        clustered_chunks[label] = []\n",
    "    clustered_chunks[label].append(chunk)\n",
    "\n",
    "# Visualizza i chunk per ciascun cluster\n",
    "for cluster, texts in clustered_chunks.items():\n",
    "    print(f\"Cluster {cluster}:\")\n",
    "    for text in texts:\n",
    "        print(f\"- {text}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b3ed5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg.special_matrices' (/storage/data_4T_b/andreacutuli/miniconda3/envs/lda-env/lib/python3.10/site-packages/scipy/linalg/special_matrices.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m corpora\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "File \u001b[0;32m~/miniconda3/envs/lda-env/lib/python3.10/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.2.0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lda-env/lib/python3.10/site-packages/gensim/corpora/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lda-env/lib/python3.10/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[1;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[0;32m~/miniconda3/envs/lda-env/lib/python3.10/site-packages/gensim/interfaces.py:19\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[1;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[0;32m~/miniconda3/envs/lda-env/lib/python3.10/site-packages/gensim/matutils.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial_matrices\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m triu\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n\u001b[1;32m     26\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg.special_matrices' (/storage/data_4T_b/andreacutuli/miniconda3/envs/lda-env/lib/python3.10/site-packages/scipy/linalg/special_matrices.py)"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Pulizia del testo (simile a quanto fatto prima)\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "# Preprocessamento del testo (tokenizzazione)\n",
    "tokenized_chunks = [word_tokenize(clean_text(chunk)) for chunk in chunks]\n",
    "\n",
    "# Creazione del dizionario e del corpus\n",
    "dictionary = corpora.Dictionary(tokenized_chunks)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_chunks]\n",
    "\n",
    "# Creazione del modello LDA\n",
    "lda_model = gensim.models.LdaMulticore(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
    "\n",
    "# Visualizza i topic generati\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Assegna i topic ai chunk\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}: {chunk}\")\n",
    "    print(f\"Topic: {lda_model.get_document_topics(corpus[i])}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8747bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
