{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfa48b46",
   "metadata": {},
   "source": [
    "# Pipeline per estrarre tutte le tabelle e le immagini da un pdf e poi tutte le immagini e OCR ocr tramite ollama modello granite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2bc6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "_log = logging.getLogger(__name__)\n",
    "\n",
    "IMAGE_RESOLUTION_SCALE = 2.0\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # cartella PDF in input\n",
    "    pdf_folder = Path(\"/storage/data_4T_b/andreacutuli/PROVA/Documents/pdf_fac_simile\")\n",
    "    # cartella radice dove verranno create le sottocartelle di output\n",
    "    output_root = Path(\"/storage/data_4T_b/andreacutuli/PROVA/Documents/output_images\")\n",
    "\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "    pipeline_options.generate_page_images = True\n",
    "    pipeline_options.generate_picture_images = True\n",
    "\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start_time_global = time.time()\n",
    "\n",
    "    # Itera su tutti i PDF nella cartella\n",
    "    for input_doc_path in pdf_folder.glob(\"*.pdf\"):\n",
    "        if not input_doc_path.exists():\n",
    "            _log.warning(f\"PDF non trovato: {input_doc_path}\")\n",
    "            continue\n",
    "\n",
    "        # Crea la sottocartella di output per questo file\n",
    "        doc_filename = input_doc_path.stem\n",
    "        output_dir = output_root / doc_filename\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        conv_res = doc_converter.convert(str(input_doc_path))\n",
    "\n",
    "        # --- Da qui in poi √® TUTTO il tuo codice originale, invariato ---\n",
    "\n",
    "        # Save page images\n",
    "        pages_obj = getattr(conv_res.document, \"pages\", None)\n",
    "        if pages_obj is None:\n",
    "            _log.warning(\"Nessuna propriet√† pages trovata su conv_res.document\")\n",
    "        else:\n",
    "            try:\n",
    "                iterator = list(pages_obj.items())  # dict-like\n",
    "                is_dict = True\n",
    "            except Exception:\n",
    "                iterator = list(enumerate(pages_obj, start=1))  # list-like\n",
    "                is_dict = False\n",
    "\n",
    "            for key, page in iterator:\n",
    "                page_no = getattr(page, \"page_no\", None) or (key if is_dict else key)\n",
    "                page_image_filename = output_dir / f\"{doc_filename}-page-{page_no}.png\"\n",
    "                try:\n",
    "                    page_image = getattr(page, \"image\", None)\n",
    "                    if page_image is None:\n",
    "                        _log.debug(f\"Nessuna immagine pagina per page {page_no}\")\n",
    "                        continue\n",
    "                    pil_img = getattr(page_image, \"pil_image\", None)\n",
    "                    if pil_img is not None:\n",
    "                        pil_img.save(page_image_filename, format=\"PNG\")\n",
    "                    else:\n",
    "                        page_image.save(str(page_image_filename), format=\"PNG\")\n",
    "                except Exception as e:\n",
    "                    _log.exception(f\"Errore salvataggio immagine pagina {page_no}: {e}\")\n",
    "\n",
    "        # Save images of figures and tables\n",
    "        table_counter = 0\n",
    "        picture_counter = 0\n",
    "        try:\n",
    "            iterator = conv_res.document.iterate_items()\n",
    "        except Exception:\n",
    "            iterator = []\n",
    "\n",
    "        for element, _level in iterator:\n",
    "            try:\n",
    "                if isinstance(element, TableItem):\n",
    "                    table_counter += 1\n",
    "                    element_image_filename = output_dir / f\"{doc_filename}-table-{table_counter}.png\"\n",
    "                    img = element.get_image(conv_res.document)\n",
    "                    if hasattr(img, \"save\"):\n",
    "                        img.save(element_image_filename, format=\"PNG\")\n",
    "                    else:\n",
    "                        with open(element_image_filename, \"wb\") as fp:\n",
    "                            fp.write(img)\n",
    "                elif isinstance(element, PictureItem):\n",
    "                    picture_counter += 1\n",
    "                    element_image_filename = output_dir / f\"{doc_filename}-picture-{picture_counter}.png\"\n",
    "                    img = element.get_image(conv_res.document)\n",
    "                    if hasattr(img, \"save\"):\n",
    "                        img.save(element_image_filename, format=\"PNG\")\n",
    "                    else:\n",
    "                        with open(element_image_filename, \"wb\") as fp:\n",
    "                            fp.write(img)\n",
    "            except Exception as e:\n",
    "                _log.exception(f\"Errore salvataggio elemento {type(element)}: {e}\")\n",
    "\n",
    "        # Generazione manuale di markdown con segnaposto\n",
    "        md_lines = []\n",
    "        table_counter_md = 0\n",
    "        picture_counter_md = 0\n",
    "\n",
    "        try:\n",
    "            iterator = conv_res.document.iterate_items()\n",
    "        except Exception:\n",
    "            iterator = []\n",
    "\n",
    "        for element, _level in iterator:\n",
    "            try:\n",
    "                if isinstance(element, TableItem):\n",
    "                    table_counter_md += 1\n",
    "                    md_lines.append(f\"[[TABLE-{table_counter_md}]]\\n\")\n",
    "                elif isinstance(element, PictureItem):\n",
    "                    picture_counter_md += 1\n",
    "                    md_lines.append(f\"[[IMAGE-{picture_counter_md}]]\\n\")\n",
    "                else:\n",
    "                    testo = getattr(element, \"text\", \"\") or getattr(element, \"get_text\", lambda: \"\")()\n",
    "                    if testo:\n",
    "                        md_lines.append(testo + \"\\n\")\n",
    "            except Exception as e:\n",
    "                _log.exception(f\"Errore processando elemento {type(element)}: {e}\")\n",
    "\n",
    "        md_placeholder_filename = output_dir / f\"{doc_filename}-with-placeholders.md\"\n",
    "        try:\n",
    "            with open(md_placeholder_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\\n\".join(md_lines))\n",
    "            print(f\"Markdown con segnaposto salvato in: {md_placeholder_filename}\")\n",
    "        except Exception as e:\n",
    "            _log.exception(f\"Errore salvataggio markdown con segnaposto: {e}\")\n",
    "\n",
    "        end_time = time.time() - start_time\n",
    "        _log.info(f\"Document converted and figures exported in {end_time:.2f} seconds.\")\n",
    "        _log.info(f\"Saved pages: {page_no if 'page_no' in locals() else 'n/a'}, tables: {table_counter}, pictures: {picture_counter}\")\n",
    "\n",
    "        for nome_file in os.listdir(output_dir):\n",
    "            percorso_file = os.path.join(output_dir, nome_file)\n",
    "            if os.path.isfile(percorso_file) and \"page\" in nome_file:\n",
    "                os.remove(percorso_file)\n",
    "                print(f\"Cancellato: {nome_file}\")\n",
    "\n",
    "    total_time = time.time() - start_time_global\n",
    "    _log.info(f\"Tutti i PDF elaborati in {total_time:.2f} secondi.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d06fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPOSTA TUTTI I FILE MARKDOWN IN UNA CARTELLA A PARTE\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Cartella sorgente\n",
    "source_dir = \"/storage/data_4T_b/andreacutuli/PROVA/Documents/output_images\"\n",
    "\n",
    "# Cartella di destinazione\n",
    "dest_dir = \"/storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_placeholders\"\n",
    "os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "# Itera ricorsivamente tutte le sottocartelle\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".md\"):\n",
    "            source_path = os.path.join(root, file)\n",
    "            dest_path = os.path.join(dest_dir, file)\n",
    "\n",
    "            # Sposta il file (sovrascrive se esiste gi√†)\n",
    "            shutil.move(source_path, dest_path)\n",
    "            print(f\"Spostato: {source_path} -> {dest_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESSA TUTTE LE IMMAGINI CON OLLAMA E SALVA LE DESCRIZIONI IN FILE .TXT\n",
    "\n",
    "import os\n",
    "import ollama\n",
    "\n",
    "input_root = \"/storage/data_4T_b/andreacutuli/PROVA/Documents/output_images\"\n",
    "descrizioni_root = \"/storage/data_4T_b/andreacutuli/PROVA/Documents/descrizioni\"\n",
    "os.makedirs(descrizioni_root, exist_ok=True)\n",
    "\n",
    "# Modello Ollama\n",
    "model = \"granite3.2-vision\"\n",
    "prompt = \"Describe what's in this image.\"\n",
    "\n",
    "# Itera sulle sottocartelle nella cartella principale di input\n",
    "for subfolder in os.listdir(input_root):\n",
    "    subfolder_path = os.path.join(input_root, subfolder)\n",
    "    if not os.path.isdir(subfolder_path):\n",
    "        continue  # salta eventuali file direttamente in input_root\n",
    "\n",
    "    # Crea la sottocartella corrispondente per le descrizioni\n",
    "    output_subfolder = os.path.join(descrizioni_root, subfolder)\n",
    "    os.makedirs(output_subfolder, exist_ok=True)\n",
    "\n",
    "    # Itera su ogni immagine nella sottocartella\n",
    "    for nome_file in os.listdir(subfolder_path):\n",
    "        percorso_file = os.path.join(subfolder_path, nome_file)\n",
    "\n",
    "        if os.path.isfile(percorso_file) and nome_file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            print(f\"\\nProcessing image: {nome_file} in folder {subfolder}\")\n",
    "\n",
    "            # Esegui inferenza con Ollama\n",
    "            response = ollama.generate(\n",
    "                model=model,\n",
    "                prompt=prompt,\n",
    "                images=[percorso_file]\n",
    "            )\n",
    "\n",
    "            descrizione = response['response']\n",
    "            print(\"Description:\", descrizione)\n",
    "\n",
    "            # Salva la descrizione in file .txt\n",
    "            txt_filename = os.path.splitext(nome_file)[0] + \".txt\"\n",
    "            txt_path = os.path.join(output_subfolder, txt_filename)\n",
    "            with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(descrizione)\n",
    "            print(f\"[OK] Descrizione salvata in {txt_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aed8e1",
   "metadata": {},
   "source": [
    "# Conversione pdf in markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da85117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 2-with-placeholders_chunks.md (13 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 21-with-placeholders_chunks.md (3 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 18-with-placeholders_chunks.md (5 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE Validazione-with-placeholders_chunks.md (5 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 14-with-placeholders_chunks.md (8 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 12-with-placeholders_chunks.md (4 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 17-with-placeholders_chunks.md (6 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 19-with-placeholders_chunks.md (8 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 8-with-placeholders_chunks.md (6 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 20-with-placeholders_chunks.md (5 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 11-with-placeholders_chunks.md (6 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 13-with-placeholders_chunks.md (9 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 3-with-placeholders_chunks.md (13 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 1-with-placeholders_chunks.md (13 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 5-with-placeholders_chunks.md (3 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 7-with-placeholders_chunks.md (3 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 1.1-with-placeholders_chunks.md (13 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 15-with-placeholders_chunks.md (5 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 10-with-placeholders_chunks.md (5 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 4-with-placeholders_chunks.md (7 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 6-with-placeholders_chunks.md (3 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 9-with-placeholders_chunks.md (6 chunk)\n",
      "‚úÖ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/FAC-SIMILE 16-with-placeholders_chunks.md (3 chunk)\n",
      "\n",
      "‚úÖ Tutti i documenti processati.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# === CONFIG ===\n",
    "SOURCE_FOLDER = \"/storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_placeholders\"  # da dove prendere i .md\n",
    "OUTPUT_FOLDER = \"/storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunks\"  # dove salvare i .md\n",
    "\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# === Inizializza chunker ===\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings=embedding_model,\n",
    "    breakpoint_threshold_type=\"gradient\"\n",
    ")\n",
    "\n",
    "# === Funzione per processare un singolo documento ===\n",
    "def process_document(file_path, output_folder):\n",
    "    # Converti in docling document\n",
    "   with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    doc_markdown = f.read()\n",
    "\n",
    "\n",
    "    # Crea chunk semantici\n",
    "    semantic_chunks = semantic_chunker.create_documents([doc_markdown])\n",
    "\n",
    "    # Salva chunk in file markdown\n",
    "    file_name = os.path.basename(file_path)\n",
    "    base_name = os.path.splitext(file_name)[0]\n",
    "    output_file = os.path.join(output_folder, f\"{base_name}_chunks.md\")\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, doc in enumerate(semantic_chunks):\n",
    "            f.write(f\"### Chunk {i + 1}\\n\\n\")\n",
    "            f.write(doc.page_content.strip() + \"\\n\\n\")\n",
    "    print(f\"‚úÖ File salvato: {output_file} ({len(semantic_chunks)} chunk)\")\n",
    "\n",
    "# === Esecuzione per tutti i file della cartella ===\n",
    "for file_name in os.listdir(SOURCE_FOLDER):\n",
    "    file_path = os.path.join(SOURCE_FOLDER, file_name)\n",
    "    if os.path.isfile(file_path) and file_name.lower().endswith('.md'):\n",
    "        process_document(file_path, OUTPUT_FOLDER)\n",
    "\n",
    "print(\"\\n‚úÖ Tutti i documenti processati.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca45fad",
   "metadata": {},
   "source": [
    "# APPLICAZIONE TABELLE E IMMAGINI DENTRO I PLACEHOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f86c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 6.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 7.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 2.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 9.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 19.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 1.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 14.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 4.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 15.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 17.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 18.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 1.1.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 21.md\n",
      "Manca contenuto per [[IMAGE-1]] (file base: FAC-SIMILE 13)\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 13.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 16.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 10.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 8.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 20.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 5.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 11.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 3.md\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE Validazione.md\n",
      "Manca contenuto per [[IMAGE-1]] (file base: FAC-SIMILE 12)\n",
      "Creato /storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image/FAC-SIMILE 12.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# cartelle principali\n",
    "template_dir = \"/storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunks\"      # markdown con placeholder\n",
    "content_dir = \"/storage/data_4T_b/andreacutuli/PROVA/Documents/descrizioni\"       # cartella con sottocartelle di tabelle/immagini\n",
    "output_dir = \"/storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image\" # dove scrivere i risultati\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# regex per i placeholder del tipo [[IMAGE-1]] [[TABLE-3]]\n",
    "placeholder_pattern = re.compile(r\"\\[\\[(IMAGE|TABLE)-(\\d+)\\]\\]\")\n",
    "\n",
    "def find_content_file(base_name, kind, num):\n",
    "    \"\"\"\n",
    "    Cerca ricorsivamente nei contenuti un file che corrisponde al pattern:\n",
    "    base_name + \"-\" + tipo + \"-\" + num + \".txt\"\n",
    "    dove tipo √® in minuscolo (table/picture).\n",
    "    \"\"\"\n",
    "    kind_map = {\"IMAGE\": \"picture\", \"TABLE\": \"table\"}\n",
    "    expected_name = f\"{base_name}-{kind_map[kind]}-{num}.txt\"\n",
    "\n",
    "    for root, _, files in os.walk(content_dir):\n",
    "        for f in files:\n",
    "            if f == expected_name:\n",
    "                return os.path.join(root, f)\n",
    "    return None\n",
    "\n",
    "for filename in os.listdir(template_dir):\n",
    "    if filename.endswith(\"-with-placeholders_chunks.md\"):\n",
    "        template_path = os.path.join(template_dir, filename)\n",
    "\n",
    "        # estraggo il \"base_name\" eliminando \"-with-placeholders_chunks.md\"\n",
    "        base_name = filename.replace(\"-with-placeholders_chunks.md\", \"\")\n",
    "\n",
    "        with open(template_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # cerco i placeholder nel testo\n",
    "        matches = placeholder_pattern.findall(content)\n",
    "\n",
    "        for kind, num in matches:\n",
    "            file_path = find_content_file(base_name, kind, num)\n",
    "            placeholder = f\"[[{kind}-{num}]]\"\n",
    "\n",
    "            if file_path:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    replacement = f.read()\n",
    "\n",
    "                # creo il blocco completo\n",
    "                wrapped = f\"{placeholder} START\\n{replacement}\\n{placeholder} END\"\n",
    "                content = content.replace(placeholder, wrapped)\n",
    "            else:\n",
    "                print(f\"Manca contenuto per {placeholder} (file base: {base_name})\")\n",
    "\n",
    "        # salvo l'output\n",
    "        out_path = os.path.join(output_dir, f\"{base_name}.md\")\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        print(f\"Creato {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626fe44",
   "metadata": {},
   "source": [
    "# RETRIEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e85d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Indicizzazione completata per FAC-SIMILE 12.md\n",
      "‚úÖ Indicizzazione completata per FAC-SIMILE 4.md\n",
      "‚úÖ Indicizzazione completata per FAC-SIMILE 18.md\n",
      "‚úÖ Indicizzazione completata per FAC-SIMILE 21.md\n",
      "‚úÖ Indicizzazione completata per FAC-SIMILE 15.md\n",
      "‚úÖ Indicizzazione completata per FAC-SIMILE 20.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "import requests\n",
    "import hashlib\n",
    "\n",
    "# === CONFIG ===\n",
    "MD_FOLDER = \"/storage/data_4T_b/andreacutuli/PROVA/Documents/markdown_chunk_placeholders_table_image\"  # cartella con file .md\n",
    "#CHROMA_PATH = \"./chroma_docs_db\"\n",
    "CHROMA_PATH = os.path.expanduser(\"~/chroma_docs_db\")\n",
    "os.makedirs(CHROMA_PATH, exist_ok=True)\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "LLM_MODEL = \"mistral\"\n",
    "OLLAMA_URL = \"http://localhost:11434/api\"\n",
    "\n",
    "# === Setup ChromaDB ===\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection_name = \"document_qa_collection\"\n",
    "collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "# === FUNZIONI ===\n",
    "def get_ollama_embedding(text):\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/embeddings\",\n",
    "            json={\"model\": EMBED_MODEL, \"prompt\": text}\n",
    "        ).json()\n",
    "        if \"embedding\" not in response:\n",
    "            raise KeyError(\"Chiave 'embedding' mancante nella risposta di Ollama.\")\n",
    "        return response[\"embedding\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Errore generando embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_file_hash(filepath):\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "def generate_response(question, relevant_chunks):\n",
    "    context = \"\\n\\n\".join(relevant_chunks)\n",
    "    prompt = (\n",
    "        \"You are an assistant for question-answering tasks. Use the following pieces of \"\n",
    "        \"retrieved context to answer the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep it concise.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\nQuestion:\\n{question}\"\n",
    "    )\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/chat\",\n",
    "            json={\n",
    "                \"model\": LLM_MODEL,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Errore durante la generazione della risposta: {e}\"\n",
    "\n",
    "def query_documents(question, n_results=3):\n",
    "    query_embedding = get_ollama_embedding(question)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=['metadatas', 'documents']\n",
    "    )\n",
    "    retrieved = []\n",
    "    if \"documents\" in results and results[\"documents\"]:\n",
    "        for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "            retrieved.append({\"text\": doc, \"filename\": meta.get('filename', 'Sconosciuto')})\n",
    "    return retrieved\n",
    "\n",
    "def index_md_file(md_filepath):\n",
    "    \"\"\"Indicizza un singolo file Markdown gi√† diviso in chunk ### Chunk, evitando re-indicizzazione se non cambia.\"\"\"\n",
    "    with open(md_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    file_hash = calculate_file_hash(md_filepath)\n",
    "    filename = os.path.basename(md_filepath)\n",
    "\n",
    "    # Controlla se il file √® gi√† presente e non √® cambiato\n",
    "    existing_docs = collection.get(where={\"filename\": filename}, include=[\"metadatas\"])\n",
    "    if existing_docs and \"metadatas\" in existing_docs and existing_docs[\"metadatas\"]:\n",
    "        stored_hashes = {meta.get(\"file_hash\") for meta in existing_docs[\"metadatas\"] if \"file_hash\" in meta}\n",
    "        if file_hash in stored_hashes:\n",
    "            print(f\"‚úîÔ∏è '{filename}' non √® cambiato. Saltato.\")\n",
    "            return\n",
    "        else:\n",
    "            # Elimina vecchi chunk del file se √® cambiato\n",
    "            collection.delete(where={\"filename\": filename})\n",
    "            print(f\"üîÑ '{filename}' modificato. Re-indicizzazione in corso...\")\n",
    "\n",
    "    raw_chunks = [chunk.strip() for chunk in content.split(\"### Chunk\") if chunk.strip()]\n",
    "    \n",
    "    for i, chunk_text in enumerate(raw_chunks):\n",
    "        chunk_id = f\"{filename}_chunk{i+1}\"\n",
    "        embedding = get_ollama_embedding(chunk_text)\n",
    "        if embedding is None:\n",
    "            print(f\"‚ö†Ô∏è Embedding fallito per {chunk_id}\")\n",
    "            continue\n",
    "        collection.upsert(\n",
    "            ids=[chunk_id],\n",
    "            documents=[chunk_text],\n",
    "            embeddings=[embedding],\n",
    "            metadatas={\"filename\": filename, \"file_hash\": file_hash}\n",
    "        )\n",
    "    print(f\"‚úÖ Indicizzazione completata per {filename}\")\n",
    "\n",
    "# === ESECUZIONE PRINCIPALE: indicizza tutti i file della cartella ===\n",
    "for file_name in os.listdir(MD_FOLDER):\n",
    "    file_path = os.path.join(MD_FOLDER, file_name)\n",
    "    if os.path.isfile(file_path) and file_name.lower().endswith(\".md\"):\n",
    "        index_md_file(file_path)\n",
    "\n",
    "# === QUERY E RISPOSTA ===\n",
    "question = \"pRischi, Guadagni Attesi e Capitale Minimo di Investimento nei vari profili di investimento\"\n",
    "retrieved_results = query_documents(question)\n",
    "\n",
    "if not retrieved_results:\n",
    "    print(\"‚ö†Ô∏è Nessun documento rilevante trovato.\")\n",
    "else:\n",
    "    relevant_chunks = [res[\"text\"] for res in retrieved_results]\n",
    "    answer = generate_response(question, relevant_chunks)\n",
    "    \n",
    "    print(\"\\n‚úÖ Risposta sintetica:\")\n",
    "    print(answer)\n",
    "    \n",
    "    source_files = sorted(list(set(res[\"filename\"] for res in retrieved_results)))\n",
    "    print(\"\\nüìö Fonti utilizzate:\")\n",
    "    for filename in source_files:\n",
    "        print(f\"- {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docling_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
