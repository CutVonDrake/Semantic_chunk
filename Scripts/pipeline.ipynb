{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da85117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from docling.document_converter import DocumentConverter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# === CONFIG ===\n",
    "SOURCE_FOLDER = \"/storage/data_4T_b/andreacutuli/PROVA/Documents/pdf_doc-fac-simile\"  # cartella con PDF/Word/Doc\n",
    "OUTPUT_FOLDER = \"/storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks\"  # dove salvare i .md\n",
    "\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# === Inizializza converter e chunker ===\n",
    "converter = DocumentConverter()\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings=embedding_model,\n",
    "    breakpoint_threshold_type=\"gradient\"\n",
    ")\n",
    "\n",
    "# === Funzione per processare un singolo documento ===\n",
    "def process_document(file_path, output_folder):\n",
    "    # Converti in docling document\n",
    "    result = converter.convert(file_path)\n",
    "    doc_markdown = result.document.export_to_markdown()\n",
    "\n",
    "    # Crea chunk semantici\n",
    "    semantic_chunks = semantic_chunker.create_documents([doc_markdown])\n",
    "\n",
    "    # Salva chunk in file markdown\n",
    "    file_name = os.path.basename(file_path)\n",
    "    base_name = os.path.splitext(file_name)[0]\n",
    "    output_file = os.path.join(output_folder, f\"{base_name}_chunks.md\")\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, doc in enumerate(semantic_chunks):\n",
    "            f.write(f\"### Chunk {i + 1}\\n\\n\")\n",
    "            f.write(doc.page_content.strip() + \"\\n\\n\")\n",
    "    print(f\"‚úÖ File salvato: {output_file} ({len(semantic_chunks)} chunk)\")\n",
    "\n",
    "# === Esecuzione per tutti i file della cartella ===\n",
    "for file_name in os.listdir(SOURCE_FOLDER):\n",
    "    file_path = os.path.join(SOURCE_FOLDER, file_name)\n",
    "    if os.path.isfile(file_path) and file_name.lower().endswith(('.pdf', '.docx', '.doc')):\n",
    "        process_document(file_path, OUTPUT_FOLDER)\n",
    "\n",
    "print(\"\\n‚úÖ Tutti i documenti processati.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8e85d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è '1 - Copia (4)_chunks.md' non √® cambiato. Saltato.\n",
      "‚úîÔ∏è '1 - Copia_chunks.md' non √® cambiato. Saltato.\n",
      "‚úîÔ∏è '1 - Copia (5)_chunks.md' non √® cambiato. Saltato.\n",
      "‚úîÔ∏è '1 - Copia (3)_chunks.md' non √® cambiato. Saltato.\n",
      "‚úîÔ∏è '1 - Copia (6)_chunks.md' non √® cambiato. Saltato.\n",
      "‚úîÔ∏è '1_chunks.md' non √® cambiato. Saltato.\n",
      "‚úîÔ∏è '1 - Copia (2)_chunks.md' non √® cambiato. Saltato.\n",
      "‚úîÔ∏è '1 - Copia (8)_chunks.md' non √® cambiato. Saltato.\n",
      "‚úîÔ∏è '1 - Copia (9)_chunks.md' non √® cambiato. Saltato.\n",
      "‚úîÔ∏è '1 - Copia (7)_chunks.md' non √® cambiato. Saltato.\n",
      "\n",
      "‚úÖ Risposta sintetica:\n",
      " The rising global temperatures, primarily due to human-induced greenhouse gas emissions, have already produced significant impacts on ecosystems and societies. Rapid retreat of glaciers, rising sea levels, and increasingly frequent extreme weather events such as hurricanes, droughts, and floods are among the alterations with direct implications on agriculture, water supply, and biodiversity. Apart from environmental damage, climate change also brings economic and social consequences, exacerbating inequalities between regions and more vulnerable populations. To address this crisis, coordinated global efforts are required, including emission reduction, transition to renewable energy sources, reforestation programs, low-emission technologies, and local adaptation policies. Public awareness and environmental education play a crucial role in fostering sustainable behaviors and supporting necessary political decisions for planet protection.\n",
      "\n",
      "üìö Fonti utilizzate:\n",
      "- 1 - Copia (4)_chunks.md\n",
      "- 1 - Copia_chunks.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "import requests\n",
    "import hashlib\n",
    "\n",
    "# === CONFIG ===\n",
    "MD_FOLDER = r\"/storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks\"  # cartella con file .md\n",
    "#CHROMA_PATH = \"./chroma_test_db\"\n",
    "CHROMA_PATH = os.path.expanduser(\"~/chroma_test_db\")\n",
    "os.makedirs(CHROMA_PATH, exist_ok=True)\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "LLM_MODEL = \"mistral\"\n",
    "OLLAMA_URL = \"http://localhost:11434/api\"\n",
    "\n",
    "# === Setup ChromaDB ===\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection_name = \"document_qa_collection\"\n",
    "collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "# === FUNZIONI ===\n",
    "def get_ollama_embedding(text):\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/embeddings\",\n",
    "            json={\"model\": EMBED_MODEL, \"prompt\": text}\n",
    "        ).json()\n",
    "        if \"embedding\" not in response:\n",
    "            raise KeyError(\"Chiave 'embedding' mancante nella risposta di Ollama.\")\n",
    "        return response[\"embedding\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Errore generando embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_file_hash(filepath):\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "def generate_response(question, relevant_chunks):\n",
    "    context = \"\\n\\n\".join(relevant_chunks)\n",
    "    prompt = (\n",
    "        \"You are an assistant for question-answering tasks. Use the following pieces of \"\n",
    "        \"retrieved context to answer the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep it concise.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\nQuestion:\\n{question}\"\n",
    "    )\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/chat\",\n",
    "            json={\n",
    "                \"model\": LLM_MODEL,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Errore durante la generazione della risposta: {e}\"\n",
    "\n",
    "def query_documents(question, n_results=3):\n",
    "    query_embedding = get_ollama_embedding(question)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=['metadatas', 'documents']\n",
    "    )\n",
    "    retrieved = []\n",
    "    if \"documents\" in results and results[\"documents\"]:\n",
    "        for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "            retrieved.append({\"text\": doc, \"filename\": meta.get('filename', 'Sconosciuto')})\n",
    "    return retrieved\n",
    "\n",
    "def index_md_file(md_filepath):\n",
    "    \"\"\"Indicizza un singolo file Markdown gi√† diviso in chunk ### Chunk, evitando re-indicizzazione se non cambia.\"\"\"\n",
    "    with open(md_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    file_hash = calculate_file_hash(md_filepath)\n",
    "    filename = os.path.basename(md_filepath)\n",
    "\n",
    "    # Controlla se il file √® gi√† presente e non √® cambiato\n",
    "    existing_docs = collection.get(where={\"filename\": filename}, include=[\"metadatas\"])\n",
    "    if existing_docs and \"metadatas\" in existing_docs and existing_docs[\"metadatas\"]:\n",
    "        stored_hashes = {meta.get(\"file_hash\") for meta in existing_docs[\"metadatas\"] if \"file_hash\" in meta}\n",
    "        if file_hash in stored_hashes:\n",
    "            print(f\"‚úîÔ∏è '{filename}' non √® cambiato. Saltato.\")\n",
    "            return\n",
    "        else:\n",
    "            # Elimina vecchi chunk del file se √® cambiato\n",
    "            collection.delete(where={\"filename\": filename})\n",
    "            print(f\"üîÑ '{filename}' modificato. Re-indicizzazione in corso...\")\n",
    "\n",
    "    raw_chunks = [chunk.strip() for chunk in content.split(\"### Chunk\") if chunk.strip()]\n",
    "    \n",
    "    for i, chunk_text in enumerate(raw_chunks):\n",
    "        chunk_id = f\"{filename}_chunk{i+1}\"\n",
    "        embedding = get_ollama_embedding(chunk_text)\n",
    "        if embedding is None:\n",
    "            print(f\"‚ö†Ô∏è Embedding fallito per {chunk_id}\")\n",
    "            continue\n",
    "        collection.upsert(\n",
    "            ids=[chunk_id],\n",
    "            documents=[chunk_text],\n",
    "            embeddings=[embedding],\n",
    "            metadatas={\"filename\": filename, \"file_hash\": file_hash}\n",
    "        )\n",
    "    print(f\"‚úÖ Indicizzazione completata per {filename}\")\n",
    "\n",
    "# === ESECUZIONE PRINCIPALE: indicizza tutti i file della cartella ===\n",
    "for file_name in os.listdir(MD_FOLDER):\n",
    "    file_path = os.path.join(MD_FOLDER, file_name)\n",
    "    if os.path.isfile(file_path) and file_name.lower().endswith(\".md\"):\n",
    "        index_md_file(file_path)\n",
    "\n",
    "# === QUERY E RISPOSTA ===\n",
    "question = \"parlami del riscaldamento globale\"\n",
    "retrieved_results = query_documents(question)\n",
    "\n",
    "if not retrieved_results:\n",
    "    print(\"‚ö†Ô∏è Nessun documento rilevante trovato.\")\n",
    "else:\n",
    "    relevant_chunks = [res[\"text\"] for res in retrieved_results]\n",
    "    answer = generate_response(question, relevant_chunks)\n",
    "    \n",
    "    print(\"\\n‚úÖ Risposta sintetica:\")\n",
    "    print(answer)\n",
    "    \n",
    "    source_files = sorted(list(set(res[\"filename\"] for res in retrieved_results)))\n",
    "    print(\"\\nüìö Fonti utilizzate:\")\n",
    "    for filename in source_files:\n",
    "        print(f\"- {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docling_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
