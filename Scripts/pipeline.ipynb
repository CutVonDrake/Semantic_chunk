{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9da85117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 19:06:52,184 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-09-30 19:06:52,185 - INFO - Load pretrained SentenceTransformer: intfloat/multilingual-e5-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 19:06:58,465 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-09-30 19:06:58,469 - INFO - Going to convert document batch...\n",
      "2025-09-30 19:06:58,470 - INFO - Initializing pipeline for StandardPdfPipeline with options hash d291d1f79894f05d312cc90dd3fdf3d3\n",
      "2025-09-30 19:06:58,470 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-09-30 19:07:02,698 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-09-30 19:07:05,077 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-09-30 19:07:06,960 - INFO - Processing document 1 - Copia (2).pdf\n",
      "2025-09-30 19:07:07,510 - INFO - Finished converting document 1 - Copia (2).pdf in 9.05 sec.\n",
      "2025-09-30 19:07:07,882 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-09-30 19:07:07,884 - INFO - Going to convert document batch...\n",
      "2025-09-30 19:07:07,885 - INFO - Processing document 1 - Copia (4).pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/1 - Copia (2)_chunks.md (2 chunk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 19:07:08,427 - INFO - Finished converting document 1 - Copia (4).pdf in 0.55 sec.\n",
      "2025-09-30 19:07:08,800 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-09-30 19:07:08,802 - INFO - Going to convert document batch...\n",
      "2025-09-30 19:07:08,803 - INFO - Processing document 1 - Copia (8).pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/1 - Copia (4)_chunks.md (2 chunk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 19:07:09,336 - INFO - Finished converting document 1 - Copia (8).pdf in 0.54 sec.\n",
      "2025-09-30 19:07:09,720 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-09-30 19:07:09,721 - INFO - Going to convert document batch...\n",
      "2025-09-30 19:07:09,722 - INFO - Processing document 1 - Copia (5).pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/1 - Copia (8)_chunks.md (2 chunk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 19:07:10,254 - INFO - Finished converting document 1 - Copia (5).pdf in 0.54 sec.\n",
      "2025-09-30 19:07:10,638 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-09-30 19:07:10,640 - INFO - Going to convert document batch...\n",
      "2025-09-30 19:07:10,641 - INFO - Processing document 1.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/1 - Copia (5)_chunks.md (2 chunk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 19:07:11,196 - INFO - Finished converting document 1.pdf in 0.56 sec.\n",
      "2025-09-30 19:07:11,396 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-09-30 19:07:11,398 - INFO - Going to convert document batch...\n",
      "2025-09-30 19:07:11,399 - INFO - Processing document 1 - Copia (7).pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/1_chunks.md (2 chunk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 19:07:11,939 - INFO - Finished converting document 1 - Copia (7).pdf in 0.54 sec.\n",
      "2025-09-30 19:07:12,144 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-09-30 19:07:12,145 - INFO - Going to convert document batch...\n",
      "2025-09-30 19:07:12,146 - INFO - Processing document 1 - Copia (6).pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/1 - Copia (7)_chunks.md (2 chunk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 19:07:13,504 - INFO - Finished converting document 1 - Copia (6).pdf in 1.36 sec.\n",
      "2025-09-30 19:07:13,893 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-09-30 19:07:13,894 - INFO - Going to convert document batch...\n",
      "2025-09-30 19:07:13,895 - INFO - Processing document 1 - Copia (3).pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/1 - Copia (6)_chunks.md (2 chunk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 19:07:14,409 - INFO - Finished converting document 1 - Copia (3).pdf in 0.52 sec.\n",
      "2025-09-30 19:07:14,782 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-09-30 19:07:14,784 - INFO - Going to convert document batch...\n",
      "2025-09-30 19:07:14,785 - INFO - Processing document 1 - Copia (9).pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/1 - Copia (3)_chunks.md (2 chunk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 19:07:15,327 - INFO - Finished converting document 1 - Copia (9).pdf in 0.55 sec.\n",
      "2025-09-30 19:07:15,528 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-09-30 19:07:15,530 - INFO - Going to convert document batch...\n",
      "2025-09-30 19:07:15,531 - INFO - Processing document 1 - Copia.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/1 - Copia (9)_chunks.md (2 chunk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 19:07:16,065 - INFO - Finished converting document 1 - Copia.pdf in 0.54 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File salvato: /storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks/1 - Copia_chunks.md (2 chunk)\n",
      "\n",
      "✅ Tutti i documenti processati.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from docling.document_converter import DocumentConverter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# === CONFIG ===\n",
    "SOURCE_FOLDER = \"/storage/data_4T_b/andreacutuli/PROVA/Documents/pdf_doc-fac-simile\"  # cartella con PDF/Word/Doc\n",
    "OUTPUT_FOLDER = \"/storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks\"  # dove salvare i .md\n",
    "\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# === Inizializza converter e chunker ===\n",
    "converter = DocumentConverter()\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings=embedding_model,\n",
    "    breakpoint_threshold_type=\"gradient\"\n",
    ")\n",
    "\n",
    "# === Funzione per processare un singolo documento ===\n",
    "def process_document(file_path, output_folder):\n",
    "    # Converti in docling document\n",
    "    result = converter.convert(file_path)\n",
    "    doc_markdown = result.document.export_to_markdown()\n",
    "\n",
    "    # Crea chunk semantici\n",
    "    semantic_chunks = semantic_chunker.create_documents([doc_markdown])\n",
    "\n",
    "    # Salva chunk in file markdown\n",
    "    file_name = os.path.basename(file_path)\n",
    "    base_name = os.path.splitext(file_name)[0]\n",
    "    output_file = os.path.join(output_folder, f\"{base_name}_chunks.md\")\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, doc in enumerate(semantic_chunks):\n",
    "            f.write(f\"### Chunk {i + 1}\\n\\n\")\n",
    "            f.write(doc.page_content.strip() + \"\\n\\n\")\n",
    "    print(f\"✅ File salvato: {output_file} ({len(semantic_chunks)} chunk)\")\n",
    "\n",
    "# === Esecuzione per tutti i file della cartella ===\n",
    "for file_name in os.listdir(SOURCE_FOLDER):\n",
    "    file_path = os.path.join(SOURCE_FOLDER, file_name)\n",
    "    if os.path.isfile(file_path) and file_name.lower().endswith(('.pdf', '.docx', '.doc')):\n",
    "        process_document(file_path, OUTPUT_FOLDER)\n",
    "\n",
    "print(\"\\n✅ Tutti i documenti processati.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e85d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indicizzazione completata per 1 - Copia (4)_chunks.md\n",
      "✅ Indicizzazione completata per 1 - Copia_chunks.md\n",
      "✅ Indicizzazione completata per 1 - Copia (5)_chunks.md\n",
      "✅ Indicizzazione completata per 1 - Copia (3)_chunks.md\n",
      "✅ Indicizzazione completata per 1 - Copia (6)_chunks.md\n",
      "✅ Indicizzazione completata per 1_chunks.md\n",
      "✅ Indicizzazione completata per 1 - Copia (2)_chunks.md\n",
      "✅ Indicizzazione completata per 1 - Copia (8)_chunks.md\n",
      "✅ Indicizzazione completata per 1 - Copia (9)_chunks.md\n",
      "✅ Indicizzazione completata per 1 - Copia (7)_chunks.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "import requests\n",
    "import hashlib\n",
    "\n",
    "# === CONFIG ===\n",
    "MD_FOLDER = r\"/storage/data_4T_b/andreacutuli/PROVA/Documents/Markdown_chunks\"  # cartella con file .md\n",
    "CHROMA_PATH = \"./chroma_test_db1\"\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "LLM_MODEL = \"mistral\"\n",
    "OLLAMA_URL = \"http://localhost:11434/api\"\n",
    "\n",
    "# === Setup ChromaDB ===\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection_name = \"document_qa_collection\"\n",
    "collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "# === FUNZIONI ===\n",
    "def get_ollama_embedding(text):\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/embeddings\",\n",
    "            json={\"model\": EMBED_MODEL, \"prompt\": text}\n",
    "        ).json()\n",
    "        if \"embedding\" not in response:\n",
    "            raise KeyError(\"Chiave 'embedding' mancante nella risposta di Ollama.\")\n",
    "        return response[\"embedding\"]\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Errore generando embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_response(question, relevant_chunks):\n",
    "    context = \"\\n\\n\".join(relevant_chunks)\n",
    "    prompt = (\n",
    "        \"You are an assistant for question-answering tasks. Use the following pieces of \"\n",
    "        \"retrieved context to answer the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep it concise.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\nQuestion:\\n{question}\"\n",
    "    )\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/chat\",\n",
    "            json={\n",
    "                \"model\": LLM_MODEL,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Errore durante la generazione della risposta: {e}\"\n",
    "\n",
    "def query_documents(question, n_results=3):\n",
    "    query_embedding = get_ollama_embedding(question)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=['metadatas', 'documents']\n",
    "    )\n",
    "    retrieved = []\n",
    "    if \"documents\" in results and results[\"documents\"]:\n",
    "        for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "            retrieved.append({\"text\": doc, \"filename\": meta.get('filename', 'Sconosciuto')})\n",
    "    return retrieved\n",
    "\n",
    "def index_md_file(md_filepath):\n",
    "    \"\"\"Indicizza un singolo file Markdown già diviso in chunk ### Chunk.\"\"\"\n",
    "    with open(md_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    raw_chunks = [chunk.strip() for chunk in content.split(\"### Chunk\") if chunk.strip()]\n",
    "    filename = os.path.basename(md_filepath)\n",
    "    \n",
    "    for i, chunk_text in enumerate(raw_chunks):\n",
    "        chunk_id = f\"{filename}_chunk{i+1}\"\n",
    "        embedding = get_ollama_embedding(chunk_text)\n",
    "        if embedding is None:\n",
    "            print(f\"⚠️ Embedding fallito per {chunk_id}\")\n",
    "            continue\n",
    "        collection.upsert(\n",
    "            ids=[chunk_id],\n",
    "            documents=[chunk_text],\n",
    "            embeddings=[embedding],\n",
    "            metadatas={\"filename\": filename}\n",
    "        )\n",
    "    print(f\"✅ Indicizzazione completata per {filename}\")\n",
    "\n",
    "# === ESECUZIONE PRINCIPALE: indicizza tutti i file della cartella ===\n",
    "for file_name in os.listdir(MD_FOLDER):\n",
    "    file_path = os.path.join(MD_FOLDER, file_name)\n",
    "    if os.path.isfile(file_path) and file_name.lower().endswith(\".md\"):\n",
    "        index_md_file(file_path)\n",
    "\n",
    "# === QUERY E RISPOSTA ===\n",
    "question = \"parlami del riscaldamento globale\"\n",
    "retrieved_results = query_documents(question)\n",
    "\n",
    "if not retrieved_results:\n",
    "    print(\"⚠️ Nessun documento rilevante trovato.\")\n",
    "else:\n",
    "    relevant_chunks = [res[\"text\"] for res in retrieved_results]\n",
    "    answer = generate_response(question, relevant_chunks)\n",
    "    \n",
    "    print(\"\\n✅ Risposta sintetica:\")\n",
    "    print(answer)\n",
    "    \n",
    "    source_files = sorted(list(set(res[\"filename\"] for res in retrieved_results)))\n",
    "    print(\"\\n📚 Fonti utilizzate:\")\n",
    "    for filename in source_files:\n",
    "        print(f\"- {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docling_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
