{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fbd0bd0-2b0b-42ce-9a5c-30cccbb6f1ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# RETRIEVAL OLLAMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ef360",
   "metadata": {},
   "source": [
    "## Elimina precedente database per inizializzazione pulita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6b2b486-d05e-4115-9bb4-ab80c2627421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cartella 'chroma_persistent_storage' non esiste.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "CHROMA_PATH = \"chroma_persistent_storage\"\n",
    "shutil.rmtree(CHROMA_PATH, ignore_errors=True)\n",
    "\n",
    "\n",
    "cartella = CHROMA_PATH\n",
    "\n",
    "# Verifica che la cartella esista\n",
    "if os.path.exists(cartella):\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "    print(f\"Cartella '{cartella}' eliminata con successo.\")\n",
    "else:\n",
    "    print(f\"La cartella '{cartella}' non esiste.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21a02b87-ab34-4531-85bc-560f7b05944f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avvio del processo di indicizzazione...\n",
      "âœ”ï¸  '1 - Copia.txt' non Ã¨ cambiato. Saltato.\n",
      "âœ”ï¸  '1.txt' non Ã¨ cambiato. Saltato.\n",
      "âœ”ï¸  '1 - Copia (8).txt' non Ã¨ cambiato. Saltato.\n",
      "âœ”ï¸  '1 - Copia (9).txt' non Ã¨ cambiato. Saltato.\n",
      "âœ”ï¸  '1 - Copia (2).txt' non Ã¨ cambiato. Saltato.\n",
      "âœ”ï¸  '1 - Copia (4).txt' non Ã¨ cambiato. Saltato.\n",
      "âœ”ï¸  '1 - Copia (3).txt' non Ã¨ cambiato. Saltato.\n",
      "âœ”ï¸  '1 - Copia (7).txt' non Ã¨ cambiato. Saltato.\n",
      "âœ”ï¸  '1 - Copia (6).txt' non Ã¨ cambiato. Saltato.\n",
      "âœ”ï¸  '1 - Copia (5).txt' non Ã¨ cambiato. Saltato.\n",
      "\n",
      "Indicizzazione completata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Risposta sintetica:\n",
      " The artificial intelligence (IA) is a significant technological innovation of the 21st century, capable of performing tasks previously considered exclusively human. It finds practical applications in various sectors such as healthcare, finance, transportation, and industry by automating complex processes. However, along with its efficiency, AI raises ethical and social questions like job replacement, algorithmic discrimination, and privacy protection. Research focuses not only on technological innovation but also on developing tools for responsible use and the protection of privacy. In the future, the integration of AI and human intelligence could redefine creativity, critical thinking, and human-machine collaboration entirely.\n",
      "\n",
      "ðŸ“š i documenti sono:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range in query.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 175\u001b[39m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;66;03m# --- MODIFICA 2: Gestiamo la nuova struttura dei dati recuperati ---\u001b[39;00m\n\u001b[32m    173\u001b[39m \n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# 1. Recupera la lista di dizionari (testo + nome file)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m retrieved_results = \u001b[43mquery_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m retrieved_results:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNon sono riuscito a trovare documenti rilevanti per la tua domanda.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mquery_documents\u001b[39m\u001b[34m(question, n_results)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m query_embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m results = \u001b[43mcollection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmetadatas\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Chiediamo esplicitamente metadati e documenti\u001b[39;49;00m\n\u001b[32m    100\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m retrieved_data = []\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;129;01mand\u001b[39;00m results[\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# Combiniamo i documenti e i metadati in una lista di dizionari\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/docling_env/lib/python3.11/site-packages/chromadb/api/models/Collection.py:213\u001b[39m, in \u001b[36mCollection.query\u001b[39m\u001b[34m(self, query_embeddings, query_texts, query_images, query_uris, ids, n_results, where, where_document, include)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\n\u001b[32m    169\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    170\u001b[39m     query_embeddings: Optional[\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m     ],\n\u001b[32m    188\u001b[39m ) -> QueryResult:\n\u001b[32m    189\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\u001b[39;00m\n\u001b[32m    190\u001b[39m \n\u001b[32m    191\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    210\u001b[39m \n\u001b[32m    211\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     query_request = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_and_prepare_query_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_images\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_uris\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_uris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m     query_results = \u001b[38;5;28mself\u001b[39m._client._query(\n\u001b[32m    226\u001b[39m         collection_id=\u001b[38;5;28mself\u001b[39m.id,\n\u001b[32m    227\u001b[39m         ids=query_request[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    234\u001b[39m         database=\u001b[38;5;28mself\u001b[39m.database,\n\u001b[32m    235\u001b[39m     )\n\u001b[32m    237\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_query_response(\n\u001b[32m    238\u001b[39m         response=query_results, include=query_request[\u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    239\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/docling_env/lib/python3.11/site-packages/chromadb/api/models/CollectionCommon.py:95\u001b[39m, in \u001b[36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, *args: Any, **kwargs: Any) -> T:\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     97\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/docling_env/lib/python3.11/site-packages/chromadb/api/models/CollectionCommon.py:293\u001b[39m, in \u001b[36mCollectionCommon._validate_and_prepare_query_request\u001b[39m\u001b[34m(self, query_embeddings, query_texts, query_images, query_uris, ids, n_results, where, where_document, include)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;129m@validation_context\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_and_prepare_query_request\u001b[39m(\n\u001b[32m    276\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m ) -> QueryRequest:\n\u001b[32m    292\u001b[39m     \u001b[38;5;66;03m# Unpack\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     query_records = \u001b[43mnormalize_base_record_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43muris\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_uris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m     filter_ids = maybe_cast_one_to_many(ids)\n\u001b[32m    302\u001b[39m     filters = FilterSet(\n\u001b[32m    303\u001b[39m         where=where,\n\u001b[32m    304\u001b[39m         where_document=where_document,\n\u001b[32m    305\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/docling_env/lib/python3.11/site-packages/chromadb/api/types.py:251\u001b[39m, in \u001b[36mnormalize_base_record_set\u001b[39m\u001b[34m(embeddings, documents, images, uris)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnormalize_base_record_set\u001b[39m(\n\u001b[32m    241\u001b[39m     embeddings: Optional[Union[OneOrMany[Embedding], OneOrMany[PyEmbedding]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    242\u001b[39m     documents: Optional[OneOrMany[Document]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    243\u001b[39m     images: Optional[OneOrMany[Image]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    244\u001b[39m     uris: Optional[OneOrMany[URI]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    245\u001b[39m ) -> BaseRecordSet:\n\u001b[32m    246\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03m    Unpacks and normalizes the fields of a BaseRecordSet.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m BaseRecordSet(\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m         embeddings=\u001b[43mnormalize_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    252\u001b[39m         documents=maybe_cast_one_to_many(documents),\n\u001b[32m    253\u001b[39m         images=maybe_cast_one_to_many(images),\n\u001b[32m    254\u001b[39m         uris=maybe_cast_one_to_many(uris),\n\u001b[32m    255\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/docling_env/lib/python3.11/site-packages/chromadb/api/types.py:170\u001b[39m, in \u001b[36mnormalize_embeddings\u001b[39m\u001b[34m(target)\u001b[39m\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Embeddings, target)\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target[\u001b[32m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    171\u001b[39m             target[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m], \u001b[38;5;28mbool\u001b[39m\n\u001b[32m    172\u001b[39m         ):\n\u001b[32m    173\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m [np.array(row, dtype=np.float32) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m target]\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    176\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected embeddings to be a list of floats or ints, a list of lists, a numpy array, or a list of numpy arrays, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    177\u001b[39m )\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range in query."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "# === CONFIG ===\n",
    "DOCUMENTS_DIR = r\"/storage/data_4T_b/andreacutuli/PROVA/Documents/doc-fac-simile\"\n",
    "CHROMA_PATH = \"./chroma_test_db1\"\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "LLM_MODEL = \"mistral\"\n",
    "OLLAMA_URL = \"http://localhost:11434/api\"\n",
    "\n",
    "# === Setup Chroma ===\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection_name = \"document_qa_collection\"\n",
    "collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "# === FUNZIONI === (Le funzioni fino a qui restano invariate)\n",
    "\n",
    "def calculate_file_hash(filepath):\n",
    "    \"\"\"Calcola l'hash SHA-256 di un file.\"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "def split_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Divide il testo in chunk con sovrapposizione.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "def get_ollama_embedding(text):\n",
    "    \"\"\"Genera embedding via Ollama REST API.\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/embeddings\",\n",
    "            json={\"model\": EMBED_MODEL, \"prompt\": text}\n",
    "        ).json()\n",
    "        if \"embedding\" not in response:\n",
    "            raise KeyError(\"La chiave 'embedding' non Ã¨ presente nella risposta di Ollama.\")\n",
    "        return response[\"embedding\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Errore di connessione a Ollama: {e}\")\n",
    "        return None\n",
    "    except KeyError as e:\n",
    "        print(f\"Errore nella risposta JSON di Ollama: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_response(question, relevant_chunks):\n",
    "    \"\"\"Genera una risposta usando il modello LLM e i chunk rilevanti.\"\"\"\n",
    "    context = \"\\n\\n\".join(relevant_chunks)\n",
    "    prompt = (\n",
    "        \"You are an assistant for question-answering tasks. Use the following pieces of \"\n",
    "        \"retrieved context to answer the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep it concise.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\nQuestion:\\n{question}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/chat\",\n",
    "            json={\n",
    "                \"model\": LLM_MODEL,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"stream\": False\n",
    "            },\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        response_data = response.json()\n",
    "        return response_data['message']['content']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Errore durante la generazione della risposta: {e}\"\n",
    "    except KeyError:\n",
    "        return \"Errore: la risposta dal modello LLM non ha il formato atteso.\"\n",
    "\n",
    "# --- MODIFICA 1: Aggiorniamo query_documents per restituire anche i metadati ---\n",
    "def query_documents(question, n_results=3):\n",
    "    \"\"\"\n",
    "    Interroga ChromaDB, recuperando sia i chunk di testo che i loro metadati\n",
    "    per poter identificare il file di origine.\n",
    "    \"\"\"\n",
    "    query_embedding = get_ollama_embedding(question)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding], \n",
    "        n_results=n_results,\n",
    "        include=['metadatas', 'documents']  # Chiediamo esplicitamente metadati e documenti\n",
    "    )\n",
    "    \n",
    "    retrieved_data = []\n",
    "    if \"documents\" in results and results[\"documents\"]:\n",
    "        # Combiniamo i documenti e i metadati in una lista di dizionari\n",
    "        for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "            retrieved_data.append({\n",
    "                \"text\": doc,\n",
    "                \"filename\": meta.get('filename', 'Sconosciuto') # .get Ã¨ piÃ¹ sicuro\n",
    "            })\n",
    "            \n",
    "    return retrieved_data\n",
    "\n",
    "# === INDICIZZAZIONE INCREMENTALE DEI DOCUMENTI === (Invariata)\n",
    "def index_documents():\n",
    "    print(\"Avvio del processo di indicizzazione...\")\n",
    "    existing_docs_metadata = collection.get(include=[\"metadatas\"])\n",
    "    db_file_hashes = {}\n",
    "    if existing_docs_metadata and existing_docs_metadata['metadatas']:\n",
    "        for meta in existing_docs_metadata['metadatas']:\n",
    "            if 'filename' in meta and 'file_hash' in meta:\n",
    "                db_file_hashes[meta['filename']] = meta['file_hash']\n",
    "\n",
    "    disk_files = set(f for f in os.listdir(DOCUMENTS_DIR) if f.endswith(\".txt\"))\n",
    "    db_files = set(db_file_hashes.keys())\n",
    "\n",
    "    for filename in disk_files:\n",
    "        filepath = os.path.join(DOCUMENTS_DIR, filename)\n",
    "        current_hash = calculate_file_hash(filepath)\n",
    "        if filename in db_file_hashes and db_file_hashes[filename] == current_hash:\n",
    "            print(f\"âœ”ï¸  '{filename}' non Ã¨ cambiato. Saltato.\")\n",
    "            continue\n",
    "        if filename in db_file_hashes:\n",
    "            print(f\"ðŸ”„ '{filename}' Ã¨ stato modificato. Re-indicizzazione in corso...\")\n",
    "            collection.delete(where={\"filename\": filename})\n",
    "        else:\n",
    "            print(f\"âž• '{filename}' Ã¨ un nuovo file. Indicizzazione in corso...\")\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        chunks = split_text(text)\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            chunk_id = f\"{filename}_chunk{i+1}\"\n",
    "            embedding = get_ollama_embedding(chunk_text)\n",
    "            if embedding:\n",
    "                collection.upsert(\n",
    "                    ids=[chunk_id],\n",
    "                    documents=[chunk_text],\n",
    "                    embeddings=[embedding],\n",
    "                    metadatas=[{\"filename\": filename, \"file_hash\": current_hash}]\n",
    "                )\n",
    "    deleted_files = db_files - disk_files\n",
    "    if deleted_files:\n",
    "        for filename in deleted_files:\n",
    "            print(f\"âž– '{filename}' Ã¨ stato rimosso. Eliminazione dal database...\")\n",
    "            collection.delete(where={\"filename\": filename})\n",
    "    print(\"\\nIndicizzazione completata.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === ESECUZIONE PRINCIPALE ===\n",
    "\n",
    "index_documents()\n",
    "\n",
    "\n",
    "while True:\n",
    "    question = input(\"\\nInserisci la tua domanda (o 'esci' per terminare): \")\n",
    "    if question.lower() == 'esci':\n",
    "        break\n",
    "\n",
    "    # --- MODIFICA 2: Gestiamo la nuova struttura dei dati recuperati ---\n",
    "    \n",
    "    # 1. Recupera la lista di dizionari (testo + nome file)\n",
    "    retrieved_results = query_documents(question)\n",
    "    \n",
    "    if not retrieved_results:\n",
    "        print(\"Non sono riuscito a trovare documenti rilevanti per la tua domanda.\")\n",
    "        continue\n",
    "\n",
    "    # 2. Estrai solo il testo da passare al modello LLM\n",
    "    relevant_chunks = [res[\"text\"] for res in retrieved_results]\n",
    "\n",
    "\n",
    "        # Recupera la risposta del modello\n",
    "    answer = generate_response(question, relevant_chunks)\n",
    "\n",
    "    # Messaggio fisso per lâ€™elenco dei documenti\n",
    "    source_msg = \"i documenti sono:\"\n",
    "\n",
    "    print(\"\\nâœ… Risposta sintetica:\")\n",
    "    print(answer)  # qui mostri la risposta reale\n",
    "\n",
    "    print(f\"\\nðŸ“š {source_msg}\")\n",
    "for filename in source_files:\n",
    "    print(f\"- {filename}\")\n",
    "\n",
    "    source_files = sorted(list(set(res[\"filename\"] for res in retrieved_results)))\n",
    "\n",
    "    print(\"\\nðŸ“š Fonti utilizzate:\")\n",
    "    for filename in source_files:\n",
    "        print(f\"- {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7eaa69-dcfe-4728-83f0-4adeba13ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [f for f in os.listdir(DOCUMENTS_DIR) if f.endswith(\".txt\")]\n",
    "total_files = len(all_files)\n",
    "correct_retrievals = 0\n",
    "\n",
    "for file in all_files:\n",
    "    query_title = os.path.splitext(file)[0]\n",
    "\n",
    "    retrieved_results = query_documents(query_title)\n",
    "    if not retrieved_results:\n",
    "        continue\n",
    "\n",
    "    top_files = [res[\"filename\"] for res in retrieved_results[:2]]\n",
    "\n",
    "    # Controlla se almeno uno dei primi due file corrisponde al titolo della query\n",
    "    match_found = any(query_title in os.path.splitext(f)[0] for f in top_files)\n",
    "    if match_found:\n",
    "        correct_retrievals += 1\n",
    "\n",
    "# Calcola la percentuale di retrieval corretto\n",
    "if total_files > 0:\n",
    "    accuracy = (correct_retrievals / total_files) * 100\n",
    "    print(f\"\\nâœ… Percentuale di retrieval corretto: {accuracy:.2f}% ({correct_retrievals}/{total_files})\")\n",
    "else:\n",
    "    print(\"Nessun file trovato per il test di retrieval.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa86da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "694c0486-0dc6-4db1-af6b-55d183d560b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# RETRIEVAL CON EMBEDDIZZAZIONE IN PARALLELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342d23c-2ffb-4638-b085-bce740055518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import concurrent.futures\n",
    "import chromadb\n",
    "\n",
    "# === CONFIGURAZIONE ===\n",
    "DOCUMENTS_DIR = r\"C:\\Users\\user\\Desktop\\claims\"\n",
    "CHROMA_PATH = \"./chroma_persistent_storage\"\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "LLM_MODEL = \"mistral\"\n",
    "OLLAMA_URL = \"http://localhost:11434/api\"\n",
    "\n",
    "# === FUNZIONI DI UTILITÃ€ ===\n",
    "def calculate_file_hash(filepath):\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "def split_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "def get_ollama_embedding(text):\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/embeddings\",\n",
    "            json={\"model\": EMBED_MODEL, \"prompt\": text}\n",
    "        ).json()\n",
    "        return response.get(\"embedding\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la generazione embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_response(question, relevant_chunks):\n",
    "    \"\"\"Genera risposta usando LLM e chunk rilevanti.\"\"\"\n",
    "    context = \"\\n\\n\".join(relevant_chunks)\n",
    "    prompt = (\n",
    "        \"You are an assistant for question-answering tasks. Use the following pieces of \"\n",
    "        \"retrieved context to answer the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep it concise.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\nQuestion:\\n{question}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/chat\",\n",
    "            json={\n",
    "                \"model\": LLM_MODEL,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"stream\": False\n",
    "            },\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        response_data = response.json()\n",
    "        return response_data['message']['content']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Errore durante la generazione della risposta: {e}\"\n",
    "    except (KeyError, json.JSONDecodeError):\n",
    "        return \"Errore: la risposta dal modello LLM non ha il formato atteso.\"\n",
    "\n",
    "def query_documents(question, collection, n_results=3):\n",
    "    \"\"\"Interroga ChromaDB per trovare chunk rilevanti.\"\"\"\n",
    "    query_embedding = get_ollama_embedding(question)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=['metadatas', 'documents']\n",
    "    )\n",
    "\n",
    "    retrieved_data = []\n",
    "    if \"documents\" in results and results[\"documents\"]:\n",
    "        for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "            retrieved_data.append({\n",
    "                \"text\": doc,\n",
    "                \"filename\": meta.get('filename', 'Sconosciuto')\n",
    "            })\n",
    "    return retrieved_data\n",
    "\n",
    "def process_file(filepath, filename):\n",
    "    \"\"\"Elaborazione indipendente di un file. Non accede al client Chroma.\"\"\"\n",
    "    try:\n",
    "        current_hash = calculate_file_hash(filepath)\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        chunks = split_text(text)\n",
    "        ids, documents, embeddings, metadatas = [], [], [], []\n",
    "\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            embedding = get_ollama_embedding(chunk_text)\n",
    "            if embedding:\n",
    "                ids.append(f\"{filename}_chunk{i+1}\")\n",
    "                documents.append(chunk_text)\n",
    "                embeddings.append(embedding)\n",
    "                metadatas.append({\"filename\": filename, \"file_hash\": current_hash})\n",
    "\n",
    "        return (ids, documents, embeddings, metadatas) if ids else None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Errore irreversibile nel file '{filename}': {e}\")\n",
    "        return None\n",
    "\n",
    "# === FUNZIONE PRINCIPALE DI INDICIZZAZIONE ===\n",
    "def index_documents():\n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "    collection = chroma_client.get_or_create_collection(name=\"document_qa_collection\")\n",
    "\n",
    "    existing_docs_metadata = collection.get(include=[\"metadatas\"])\n",
    "    db_file_hashes = {\n",
    "        meta['filename']: meta['file_hash']\n",
    "        for meta in existing_docs_metadata.get('metadatas', [])\n",
    "        if 'filename' in meta and 'file_hash' in meta\n",
    "    }\n",
    "\n",
    "    disk_files = {f for f in os.listdir(DOCUMENTS_DIR) if f.endswith(\".txt\")}\n",
    "    db_files = set(db_file_hashes.keys())\n",
    "\n",
    "    files_to_process = []\n",
    "    for filename in disk_files:\n",
    "        filepath = os.path.join(DOCUMENTS_DIR, filename)\n",
    "        current_hash = calculate_file_hash(filepath)\n",
    "        if db_file_hashes.get(filename) == current_hash:\n",
    "            print(f\"âœ”ï¸  '{filename}' non Ã¨ cambiato. Saltato.\")\n",
    "            continue\n",
    "        if filename in db_file_hashes:\n",
    "            print(f\"ðŸ”„ '{filename}' modificato. Re-indicizzazione.\")\n",
    "            collection.delete(where={\"filename\": filename})\n",
    "        else:\n",
    "            print(f\"âž• '{filename}' Ã¨ nuovo. Indicizzazione.\")\n",
    "        files_to_process.append((filepath, filename))\n",
    "\n",
    "    all_ids, all_documents, all_embeddings, all_metadatas = [], [], [], []\n",
    "\n",
    "    if files_to_process:\n",
    "        print(f\"\\nðŸš€ Elaborazione parallela di {len(files_to_process)} file con ThreadPoolExecutor...\")\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            future_to_file = {executor.submit(process_file, fp, fn): fn for fp, fn in files_to_process}\n",
    "            for future in concurrent.futures.as_completed(future_to_file):\n",
    "                filename = future_to_file[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        ids, documents, embeddings, metadatas = result\n",
    "                        all_ids.extend(ids)\n",
    "                        all_documents.extend(documents)\n",
    "                        all_embeddings.extend(embeddings)\n",
    "                        all_metadatas.extend(metadatas)\n",
    "                except Exception as exc:\n",
    "                    print(f\"âŒ Eccezione grave durante l'elaborazione di '{filename}': {exc}\")\n",
    "\n",
    "        if all_ids:\n",
    "            print(f\"\\nâœ… Inserimento in batch di {len(all_ids)} chunk nel database...\")\n",
    "            collection.upsert(\n",
    "                ids=all_ids,\n",
    "                documents=all_documents,\n",
    "                embeddings=all_embeddings,\n",
    "                metadatas=all_metadatas\n",
    "            )\n",
    "        else:\n",
    "            print(\"\\nâš ï¸ Nessun nuovo chunk da inserire nel database.\")\n",
    "\n",
    "    deleted_files = db_files - disk_files\n",
    "    for filename in deleted_files:\n",
    "        print(f\"âž– '{filename}' rimosso dal disco. Eliminazione dal database...\")\n",
    "        collection.delete(where={\"filename\": filename})\n",
    "\n",
    "    print(\"\\nIndicizzazione completata.\")\n",
    "\n",
    "# === ESECUZIONE PRINCIPALE ===\n",
    "if __name__ == \"__main__\":\n",
    "    index_documents()\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "    collection = chroma_client.get_or_create_collection(name=\"document_qa_collection\")\n",
    "\n",
    "    while True:\n",
    "        question = input(\"\\nInserisci la tua domanda (o 'esci' per terminare): \")\n",
    "        if question.lower() == 'esci':\n",
    "            break\n",
    "\n",
    "        retrieved_results = query_documents(question, collection)\n",
    "        if not retrieved_results:\n",
    "            print(\"Non sono riuscito a trovare documenti rilevanti.\")\n",
    "            continue\n",
    "\n",
    "        relevant_chunks = [res[\"text\"] for res in retrieved_results]\n",
    "        answer = generate_response(question, relevant_chunks)\n",
    "\n",
    "        source_files = sorted(list(set(res[\"filename\"] for res in retrieved_results)))\n",
    "\n",
    "        print(\"\\nâœ… Risposta sintetica:\")\n",
    "        print(answer)\n",
    "\n",
    "        print(\"\\nðŸ“š Fonti utilizzate:\")\n",
    "        for filename in source_files:\n",
    "            print(f\"- {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ab37a6-0ebd-4e1d-b7c5-c7b789093d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(collection, top_k=2):\n",
    "    all_files = [f for f in os.listdir(DOCUMENTS_DIR) if f.endswith(\".txt\")]\n",
    "    total_files = len(all_files)\n",
    "    correct_retrievals = 0\n",
    "\n",
    "    for file in all_files:\n",
    "        query_title = os.path.splitext(file)[0]\n",
    "\n",
    "        retrieved_results = query_documents(query_title, collection, n_results=top_k)\n",
    "        if not retrieved_results:\n",
    "            continue\n",
    "\n",
    "        top_files = [res[\"filename\"] for res in retrieved_results[:top_k]]\n",
    "\n",
    "        match_found = any(query_title in os.path.splitext(f)[0] for f in top_files)\n",
    "        if match_found:\n",
    "            correct_retrievals += 1\n",
    "\n",
    "    if total_files > 0:\n",
    "        accuracy = (correct_retrievals / total_files) * 100\n",
    "        print(f\" accuracy = {accuracy:.2f}%\")\n",
    "    else:\n",
    "        print(\"0.00\")\n",
    "\n",
    "\n",
    "import sys, io\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # silenzia i print di index_documents\n",
    "    sys_stdout_backup = sys.stdout\n",
    "    sys.stdout = io.StringIO()\n",
    "    index_documents()\n",
    "    sys.stdout = sys_stdout_backup\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "    collection = chroma_client.get_or_create_collection(name=\"document_qa_collection\")\n",
    "\n",
    "    evaluate_retrieval(collection, top_k=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c48421-0ca3-497e-93cc-12445f0b5d0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DIVISIONE DOCUMENTO IN CHUNK E SINTESI DI OGNI CHUNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a94cb-dd93-43d0-91c2-46773be5b72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are_any_of_the_border_states_covered_by_the_Ninth_Circuit_Court_of_Appeals.txt: 20 chunk trovati\n",
      "Are_any_of_the_border_states_covered_by_the_Ninth_Circuit_Court_of_Appeals.txt salvato in ChromaDB con percorso.\n",
      "Are_green_house_gasses_what_cause_holes_in_the_ozone_layer.txt: 53 chunk trovati\n",
      "Are_green_house_gasses_what_cause_holes_in_the_ozone_layer.txt salvato in ChromaDB con percorso.\n",
      "Are_there_any_circumstances_where_exemptions_of_mandatory_vaccinations_in_those_provinces_for_studen.txt: 97 chunk trovati\n",
      "Are_there_any_circumstances_where_exemptions_of_mandatory_vaccinations_in_those_provinces_for_studen.txt salvato in ChromaDB con percorso.\n",
      "Did_Barack_Obama_write_any_Autobiographies_before_2019.txt: 26 chunk trovati\n",
      "Did_Barack_Obama_write_any_Autobiographies_before_2019.txt salvato in ChromaDB con percorso.\n",
      "Did_Hunter_Biden_have_any_experience_in_the_energy_sector_at_the_time_he_joined_the_board_of_the__Bu.txt: 89 chunk trovati\n",
      "Did_Hunter_Biden_have_any_experience_in_the_energy_sector_at_the_time_he_joined_the_board_of_the__Bu.txt salvato in ChromaDB con percorso.\n",
      "Did_Hunter_Biden_have_any_experience_in_Ukraine_at_the_time_he_joined_the_board_of_the__Burisma_ener.txt: 89 chunk trovati\n",
      "Did_Hunter_Biden_have_any_experience_in_Ukraine_at_the_time_he_joined_the_board_of_the__Burisma_ener.txt salvato in ChromaDB con percorso.\n",
      "Did_Kenya_build_11200_kilometres_of_tarmacked_roads_in_the_50_years_post_independence.txt: 79 chunk trovati\n",
      "Did_Kenya_build_11200_kilometres_of_tarmacked_roads_in_the_50_years_post_independence.txt salvato in ChromaDB con percorso.\n",
      "Did_Nancy_Green_have_any_other_jobs_after_her_role_as_Aunt_Jemima.txt: 11 chunk trovati\n",
      "Did_Nancy_Green_have_any_other_jobs_after_her_role_as_Aunt_Jemima.txt salvato in ChromaDB con percorso.\n",
      "Did_Sen_Bernie_Sanders_have_a_job_before_age_53.txt: 184 chunk trovati\n",
      "Did_Sen_Bernie_Sanders_have_a_job_before_age_53.txt salvato in ChromaDB con percorso.\n",
      "Did_the_Democrats_and_the_Deep_state_do_anything_to_create_the_narcotics_epidemic_in_the_USA.txt: 21 chunk trovati\n",
      "Did_the_Democrats_and_the_Deep_state_do_anything_to_create_the_narcotics_epidemic_in_the_USA.txt salvato in ChromaDB con percorso.\n",
      "Does_Adam_Schiff_have_siblings.txt: 93 chunk trovati\n",
      "Does_Adam_Schiff_have_siblings.txt salvato in ChromaDB con percorso.\n",
      "Does_Cherie_Blair_deal_with_immigration_law.txt: 22 chunk trovati\n",
      "Does_Cherie_Blair_deal_with_immigration_law.txt salvato in ChromaDB con percorso.\n",
      "Does_USA_have_a_subsidy_system_for_wind_turbines.txt: 11 chunk trovati\n",
      "Does_USA_have_a_subsidy_system_for_wind_turbines.txt salvato in ChromaDB con percorso.\n",
      "Do_foreign_governments_gets_to_pick_lottery_applicants.txt: 23 chunk trovati\n",
      "Do_foreign_governments_gets_to_pick_lottery_applicants.txt salvato in ChromaDB con percorso.\n",
      "Has_Amy_Klobuchar_won_every_election_she_has_been_in.txt: 59 chunk trovati\n",
      "Has_Amy_Klobuchar_won_every_election_she_has_been_in.txt salvato in ChromaDB con percorso.\n",
      "Has_Elizabeth_Warren_won_every_election_she_has_been_in.txt: 72 chunk trovati\n",
      "Has_Elizabeth_Warren_won_every_election_she_has_been_in.txt salvato in ChromaDB con percorso.\n",
      "Has_the_motto_of_the_Supreme_Court_of_India_been_changed.txt: 5 chunk trovati\n",
      "Has_the_motto_of_the_Supreme_Court_of_India_been_changed.txt salvato in ChromaDB con percorso.\n",
      "How_is_the_nightly_pledge_of_allegiance_recited.txt: 31 chunk trovati\n",
      "How_is_the_nightly_pledge_of_allegiance_recited.txt salvato in ChromaDB con percorso.\n",
      "How_long_did_Nancy_Green_portray_Aunt_Jemima_.txt: 11 chunk trovati\n",
      "How_long_did_Nancy_Green_portray_Aunt_Jemima_.txt salvato in ChromaDB con percorso.\n",
      "How_long_did_the_1968_Flu_pandemic_last.txt: 86 chunk trovati\n",
      "How_long_did_the_1968_Flu_pandemic_last.txt salvato in ChromaDB con percorso.\n",
      "How_many_mosques_are_in_Bangaluru_Bangalore.txt: 33 chunk trovati\n",
      "How_many_mosques_are_in_Bangaluru_Bangalore.txt salvato in ChromaDB con percorso.\n",
      "How_old_was_the_bridge_when_it_collapsed.txt: 10 chunk trovati\n",
      "How_old_was_the_bridge_when_it_collapsed.txt salvato in ChromaDB con percorso.\n",
      "How_tall_was_Grenfell_Tower.txt: 108 chunk trovati\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     82\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m salvato in ChromaDB con percorso.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[43mprocess_documents_one_by_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTutti i documenti processati e salvati.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mprocess_documents_one_by_one\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     61\u001b[39m ids, documents, metadatas, embeddings = [], [], [], []\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks, start=\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     summary = \u001b[43msummarize_with_ollama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     embedding = embed_with_ollama(summary)\n\u001b[32m     67\u001b[39m     ids.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_chunk\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36msummarize_with_ollama\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msummarize_with_ollama\u001b[39m(text):\n\u001b[32m     28\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRiepiloga in modo conciso il seguente testo:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mOLLAMA_URL\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/chat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mLLM_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     response.raise_for_status()\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\chroma_rag\\Lib\\site-packages\\requests\\api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\chroma_rag\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\chroma_rag\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\chroma_rag\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\chroma_rag\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\chroma_rag\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\chroma_rag\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\chroma_rag\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\chroma_rag\\Lib\\http\\client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\chroma_rag\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\chroma_rag\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\chroma_rag\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "\n",
    "#CONFIGURAZIONE\n",
    "DOCUMENTS_DIR = r\"C:\\Users\\user\\Desktop\\claims\"\n",
    "OLLAMA_URL = \"http://localhost:11434/api\"\n",
    "LLM_MODEL = \"mistral\"              \n",
    "EMBED_MODEL = \"nomic-embed-text\"   \n",
    "HF_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "\n",
    "CHROMA_PATH = \"./chroma_persistent_storage1\"\n",
    "\n",
    "# MODELLI EMBEDDING E CHUNKER SEMANTICO\n",
    "embedding_model_hf = HuggingFaceEmbeddings(model_name=HF_MODEL)\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings=embedding_model_hf,\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection = chroma_client.get_or_create_collection(name=\"chunk_summaries\")\n",
    "\n",
    "# FUNZIONI \n",
    "def summarize_with_ollama(text):\n",
    "    prompt = f\"Riepiloga in modo conciso il seguente testo:\\n\\n{text}\"\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_URL}/chat\",\n",
    "        json={\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"message\"][\"content\"].strip()\n",
    "\n",
    "def embed_with_ollama(text):\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_URL}/embeddings\",\n",
    "        json={\"model\": EMBED_MODEL, \"prompt\": text}\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"embedding\"]\n",
    "\n",
    "#FUNZIONE PROCESSA DOCUMENTI\n",
    "def process_documents_one_by_one():\n",
    "    for filename in os.listdir(DOCUMENTS_DIR):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "\n",
    "        filepath = os.path.join(DOCUMENTS_DIR, filename)\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        chunks = semantic_chunker.split_text(text)\n",
    "        print(f\"{filename}: {len(chunks)} chunk trovati\")\n",
    "\n",
    "        ids = []\n",
    "        documents = []\n",
    "        metadatas = []\n",
    "        embeddings = []\n",
    "\n",
    "        for i, chunk in enumerate(chunks, start=1):\n",
    "            summary = summarize_with_ollama(chunk)\n",
    "            embedding = embed_with_ollama(summary)\n",
    "\n",
    "            ids.append(f\"{filename}_chunk{i}\")\n",
    "            documents.append(summary)\n",
    "            metadatas.append({\n",
    "                \"filename\": filename,\n",
    "                \"filepath\": filepath  # aggiunto il percorso completo\n",
    "            })\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        # Salva chunk del singolo documento in ChromaDB\n",
    "        collection.upsert(\n",
    "            ids=ids,\n",
    "            documents=documents,\n",
    "            metadatas=metadatas,\n",
    "            embeddings=embeddings\n",
    "        )\n",
    "        print(f\"{filename} salvato in ChromaDB con percorso.\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_documents_one_by_one()\n",
    "    print(\"Tutti i documenti processati e salvati.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67aa690e-5444-4252-9fa6-323c8331cad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: Are_green_house_gasses_what_cause_holes_in_the_ozone_layer.txt_chunk1\n",
      "Sintesi: The text you provided is a summary of information about greenhouse gases, their sources, effects, and potential solutions to reduce emissions. Here are some key points:\n",
      "\n",
      "1. Greenhouse gases (GHGs) are gases in Earth's atmosphere that trap heat and warm the planet, leading to the greenhouse effect. Examples include carbon dioxide (CO2), methane, nitrous oxide, and fluorinated gases.\n",
      "\n",
      "2. Human activities, such as burning fossil fuels and deforestation, have significantly increased GHG emissions in the industrial era. These activities contribute to global warming and climate change.\n",
      "\n",
      "3. The greenhouse effect is responsible for making Earth's overall temperature higher than it would be without these gases. The term \"greenhouse\" was first applied to this phenomenon by Nils Gustaf Ekholm in 1901.\n",
      "\n",
      "4. Most GHGs have both natural and human-caused sources, with the exception of synthetic halocarbons which have no natural sources. Hydrofluorocarbons (HFCs) are regulated by the Montreal Protocol due to their contribution to ozone depletion rather than global warming.\n",
      "\n",
      "5. The Intergovernmental Panel on Climate Change (IPCC) warns that greenhouse gas emissions must peak before 2025 at the latest and decline 43% by 2030 to have a good chance of limiting global warming to 1.5Â°C (2.7Â°F).\n",
      "\n",
      "6. Negative emissions technologies, such as carbon capture and storage, bio-energy with carbon capture and storage, and carbon dioxide air capture, are being studied for their potential to remove GHGs from the atmosphere and mitigate climate change.\n",
      "\n",
      "7. Greenhouse gases also exist in the atmospheres of other planets like Mars, Titan, and particularly in the thick atmosphere of Venus. However, the greenhouse effect on these planets is much more extreme than on Earth, with Venus serving as an example of a runaway greenhouse effect.\n",
      "Metadati: {'filename': 'Are_green_house_gasses_what_cause_holes_in_the_ozone_layer.txt', 'filepath': 'C:\\\\Users\\\\user\\\\Desktop\\\\claims\\\\Are_green_house_gasses_what_cause_holes_in_the_ozone_layer.txt'}\n",
      "Embedding (prime 10 dimensioni): [ 0.54828095  0.92243761 -3.61421013 -0.07909562  1.34369612  0.90484154\n",
      " -0.04663147  0.33861941 -0.16989124 -0.9532057 ]\n"
     ]
    }
   ],
   "source": [
    "result = collection.get(limit=1, include=[\"documents\", \"metadatas\", \"embeddings\"])\n",
    "\n",
    "print(\"ID:\", result[\"ids\"][0])\n",
    "print(\"Sintesi:\", result[\"documents\"][0])\n",
    "print(\"Metadati:\", result[\"metadatas\"][0])\n",
    "print(\"Embedding (prime 10 dimensioni):\", result[\"embeddings\"][0][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cea89c4-bdef-40d2-8074-3f19c6ec82ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Retrieval sul testo sintetizzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded0e6f4-0ebf-491b-810e-8b42ff6ddf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inserisci la tua domanda (o 'esci' per terminare):  hunter biden\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Risposta:\n",
      " Hunter Biden is an American businessman, lawyer, and the son of U.S. President Joe Biden. Born on February 4, 1970, he has worked in various fields such as investing, lobbying, and philanthropy. He has been involved in several controversies, including investigations and federal indictments for firearms trials, tax indictment, guilty pleas, pardon of criminal offenses, laptop files, Navy Reserve issues, and litigation. In his personal life, he has had relationships and struggled with drug and alcohol abuse. He is known in popular culture for his role in the Ukraine conspiracy theory, Department of Justice investigation, laptop controversy, and the book \"Beautiful Things\" about his struggles with addiction.\n",
      "\n",
      "ðŸ“š Fonti utilizzate:\n",
      "- Did_Hunter_Biden_have_any_experience_in_Ukraine_at_the_time_he_joined_the_board_of_the__Burisma_ener.txt\n",
      "- Did_Hunter_Biden_have_any_experience_in_Ukraine_at_the_time_he_joined_the_board_of_the__Burisma_ener.txt\n",
      "- Did_Hunter_Biden_have_any_experience_in_the_energy_sector_at_the_time_he_joined_the_board_of_the__Bu.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inserisci la tua domanda (o 'esci' per terminare):  Did_Barack_Obama_write_any_Autobiographies_before_2019?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Risposta:\n",
      " No, Barack Obama did not write any autobiographies before 2019, according to the provided bibliography. The only autobiography mentioned in the context is by Sasha Abramsky, published in 2009, which is not written by Obama himself.\n",
      "\n",
      "ðŸ“š Fonti utilizzate:\n",
      "- Did_Barack_Obama_write_any_Autobiographies_before_2019.txt\n",
      "- Did_Barack_Obama_write_any_Autobiographies_before_2019.txt\n",
      "- Did_Barack_Obama_write_any_Autobiographies_before_2019.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inserisci la tua domanda (o 'esci' per terminare):  esci\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import chromadb\n",
    "\n",
    "#CONFIGURAZIONE \n",
    "CHROMA_PATH = \"./chroma_persistent_storage1\"\n",
    "OLLAMA_URL = \"http://localhost:11434/api\"\n",
    "LLM_MODEL = \"mistral\"\n",
    "TOP_K = 3  # numero di chunk da recuperare\n",
    "\n",
    "#Inizializzazone ChromaDB\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection = chroma_client.get_or_create_collection(name=\"chunk_summaries\")\n",
    "\n",
    "#ChromaDB query\n",
    "def query_chroma(question, top_k=TOP_K):\n",
    "    # recupera embedding della domanda\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_URL}/embeddings\",\n",
    "        json={\"model\": \"nomic-embed-text\", \"prompt\": question}\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    query_embedding = response.json()[\"embedding\"]\n",
    "\n",
    "    # cerca chunk piÃ¹ rilevanti\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    retrieved_chunks = []\n",
    "    if results[\"documents\"]:\n",
    "        for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "            retrieved_chunks.append({\"text\": doc, \"filename\": meta.get(\"filename\", \"\")})\n",
    "    return retrieved_chunks\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# === Funzione per generare risposta con Mistral ===\n",
    "def generate_answer(question, retrieved_chunks):\n",
    "    context = \"\\n\\n\".join([c[\"text\"] for c in retrieved_chunks])\n",
    "    prompt = (\n",
    "        f\"Usa i seguenti contesti per rispondere brevemente alla domanda in italiano.\\n\\n\"\n",
    "        f\"Contesto:\\n{context}\\n\\nDomanda:\\n{question}\"\n",
    "    )\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_URL}/chat\",\n",
    "        json={\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"Sei un assistente utile.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"message\"][\"content\"]\n",
    "\n",
    "    '''\n",
    "\n",
    "# GENERARE RETRIEVAL CON MISTRAL\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        question = input(\"\\nInserisci la tua domanda (o 'esci' per terminare): \")\n",
    "        if question.lower() == \"esci\":\n",
    "            break\n",
    "\n",
    "        chunks = query_chroma(question)\n",
    "        if not chunks:\n",
    "            print(\"Nessun chunk rilevante trovato.\")\n",
    "            continue\n",
    "\n",
    "        #answer = generate_answer(question, chunks)\n",
    "        #print(\"\\nâœ… Risposta:\")\n",
    "        #print(answer)\n",
    "\n",
    "        print(\"\\nðŸ“š Fonti utilizzate:\")\n",
    "        for c in chunks:\n",
    "            print(f\"- {c['filename']}\")\n",
    "\n",
    "\n",
    "'''\n",
    "idee da implementare: hash per evitare rielaborazione documenti non cambiati\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "343b5ff8-d1c8-4134-8dfc-fdabb7362bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Retrieval accuracy = 21.00% (21/100)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import chromadb\n",
    "\n",
    "# === CONFIG ===\n",
    "DOCUMENTS_DIR = r\"C:\\Users\\user\\Desktop\\claims\"\n",
    "CHROMA_PATH = \"./chroma_persistent_storage1\"\n",
    "OLLAMA_URL = \"http://localhost:11434/api\"\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "TOP_K = 3\n",
    "\n",
    "# === Inizializza ChromaDB ===\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection = chroma_client.get_or_create_collection(name=\"chunk_summaries\")\n",
    "\n",
    "# === Funzione per interrogare ChromaDB ===\n",
    "def query_chroma(question, top_k=TOP_K):\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_URL}/embeddings\",\n",
    "        json={\"model\": EMBED_MODEL, \"prompt\": question}\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    query_embedding = response.json()[\"embedding\"]\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    retrieved_chunks = []\n",
    "    if results[\"documents\"]:\n",
    "        for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "            retrieved_chunks.append({\"text\": doc, \"filename\": meta.get(\"filename\", \"\")})\n",
    "    return retrieved_chunks\n",
    "\n",
    "# === Funzione di valutazione retrieval ===\n",
    "def evaluate_retrieval(collection, top_k=TOP_K):\n",
    "    all_files = [f for f in os.listdir(DOCUMENTS_DIR) if f.endswith(\".txt\")]\n",
    "    total_files = len(all_files)\n",
    "    correct_retrievals = 0\n",
    "\n",
    "    for file in all_files:\n",
    "        query_title = os.path.splitext(file)[0]  # nome file senza estensione\n",
    "\n",
    "        retrieved_results = query_chroma(query_title, top_k=top_k)\n",
    "        if not retrieved_results:\n",
    "            continue\n",
    "\n",
    "        # prendi i top-k nomi file dei chunk recuperati\n",
    "        top_files = [res[\"filename\"] for res in retrieved_results[:top_k]]\n",
    "\n",
    "        # match se almeno uno contiene il titolo\n",
    "        match_found = any(query_title in os.path.splitext(f)[0] for f in top_files)\n",
    "        if match_found:\n",
    "            correct_retrievals += 1\n",
    "\n",
    "    if total_files > 0:\n",
    "        accuracy = (correct_retrievals / total_files) * 100\n",
    "        print(f\"\\nðŸ“Š Retrieval accuracy = {accuracy:.2f}% \"\n",
    "              f\"({correct_retrievals}/{total_files})\")\n",
    "    else:\n",
    "        print(\"Nessun file trovato per valutazione.\")\n",
    "\n",
    "# === Main ===\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_retrieval(collection, top_k=TOP_K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2fa273f-606d-4ebb-9401-3a515a97480d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Retrieval accuracy = 90.91% (20/22)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import chromadb\n",
    "\n",
    "# === CONFIG ===\n",
    "CHROMA_PATH = \"./chroma_persistent_storage1\"\n",
    "OLLAMA_URL = \"http://localhost:11434/api\"\n",
    "TOP_K = 2\n",
    "\n",
    "# === Inizializza ChromaDB ===\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection = chroma_client.get_or_create_collection(name=\"chunk_summaries\")\n",
    "\n",
    "# === Funzione query ===\n",
    "def query_chroma(question, top_k=TOP_K):\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_URL}/embeddings\",\n",
    "        json={\"model\": \"nomic-embed-text\", \"prompt\": question}\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    query_embedding = response.json()[\"embedding\"]\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    retrieved_chunks = []\n",
    "    if results[\"documents\"]:\n",
    "        for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "            retrieved_chunks.append({\"text\": doc, \"filename\": meta.get(\"filename\", \"\")})\n",
    "    return retrieved_chunks\n",
    "\n",
    "# === Funzione di valutazione ===\n",
    "def evaluate_retrieval(questions, collection, top_k=2):\n",
    "    total = len(questions)\n",
    "    correct = 0\n",
    "\n",
    "    for q in questions:\n",
    "        retrieved = query_chroma(q, top_k=top_k)\n",
    "        if not retrieved:\n",
    "            continue\n",
    "\n",
    "        top_files = [res[\"filename\"] for res in retrieved[:top_k]]\n",
    "        # match se il nome della domanda Ã¨ contenuto nel filename\n",
    "        match_found = any(q in os.path.splitext(f)[0] for f in top_files)\n",
    "        if match_found:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"ðŸ“Š Retrieval accuracy = {accuracy:.2f}% ({correct}/{total})\")\n",
    "\n",
    "# === Lista delle domande ===\n",
    "questions = [\n",
    "    \"Are_any_of_the_border_states_covered_by_the_Ninth_Circuit_Court_of_Appeals\",\n",
    "    \"Are_green_house_gasses_what_cause_holes_in_the_ozone_layer\",\n",
    "    \"Are_there_any_circumstances_where_exemptions_of_mandatory_vaccinations_in_those_provinces_for_studen\",\n",
    "    \"Did_Barack_Obama_write_any_Autobiographies_before_2019\",\n",
    "    \"Did_Hunter_Biden_have_any_experience_in_the_energy_sector_at_the_time_he_joined_the_board_of_the__Bu\",\n",
    "    \"Did_Hunter_Biden_have_any_experience_in_Ukraine_at_the_time_he_joined_the_board_of_the__Burisma_ener\",\n",
    "    \"Did_Kenya_build_11200_kilometres_of_tarmacked_roads_in_the_50_years_post_independence\",\n",
    "    \"Did_Nancy_Green_have_any_other_jobs_after_her_role_as_Aunt_Jemima\",\n",
    "    \"Did_Sen_Bernie_Sanders_have_a_job_before_age_53\",\n",
    "    \"Did_the_Democrats_and_the_Deep_state_do_anything_to_create_the_narcotics_epidemic_in_the_USA\",\n",
    "    \"Does_Adam_Schiff_have_siblings\",\n",
    "    \"Does_Cherie_Blair_deal_with_immigration_law\",\n",
    "    \"Does_USA_have_a_subsidy_system_for_wind_turbines\",\n",
    "    \"Do_foreign_governments_gets_to_pick_lottery_applicants\",\n",
    "    \"Has_Amy_Klobuchar_won_every_election_she_has_been_in\",\n",
    "    \"Has_Elizabeth_Warren_won_every_election_she_has_been_in\",\n",
    "    \"Has_the_motto_of_the_Supreme_Court_of_India_been_changed\",\n",
    "    \"How_is_the_nightly_pledge_of_allegiance_recited\",\n",
    "    \"How_long_did_Nancy_Green_portray_Aunt_Jemima_\",\n",
    "    \"How_long_did_the_1968_Flu_pandemic_last\",\n",
    "    \"How_many_mosques_are_in_Bangaluru_Bangalore\",\n",
    "    \"How_old_was_the_bridge_when_it_collapsed\"\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_retrieval(questions, collection, top_k=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docling_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
